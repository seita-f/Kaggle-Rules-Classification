{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":94635,"databundleVersionId":13121456,"sourceType":"competition"},{"sourceId":2795202,"sourceType":"datasetVersion","datasetId":575905},{"sourceId":6489118,"sourceType":"datasetVersion","datasetId":3749864},{"sourceId":13272190,"sourceType":"datasetVersion","datasetId":8410972},{"sourceId":270017687,"sourceType":"kernelVersion"},{"sourceId":171638,"sourceType":"modelInstanceVersion","modelInstanceId":146086,"modelId":164048}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Setup","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nSEED = 42","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:43.897071Z","iopub.execute_input":"2025-10-25T14:01:43.897344Z","iopub.status.idle":"2025-10-25T14:01:47.118449Z","shell.execute_reply.started":"2025-10-25T14:01:43.897317Z","shell.execute_reply":"2025-10-25T14:01:47.117497Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%writefile constants.py\n\n# Global\nSEED = 42\nOUT_DIR = \"/kaggle/working/\"\nBATCH_SIZE = 32\nMAX_TOKEN = 256\n\n# BERT\nBERT_MODEL_DIR = \"/kaggle/input/my-base-bert/my-bert-cls\"\nBERT_EPOCH = 3\n\n# RoBERTa\nROBERTA_MODEL_DIR = \"/kaggle/input/roberta-base\"\nROBERTA_EPOCH = 9","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:47.120138Z","iopub.execute_input":"2025-10-25T14:01:47.120609Z","iopub.status.idle":"2025-10-25T14:01:47.126717Z","shell.execute_reply.started":"2025-10-25T14:01:47.120580Z","shell.execute_reply":"2025-10-25T14:01:47.125880Z"}},"outputs":[{"name":"stdout","text":"Writing constants.py\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"# Augmentation","metadata":{}},{"cell_type":"code","source":"df_train = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/train.csv\")\ndf_test = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:47.127811Z","iopub.execute_input":"2025-10-25T14:01:47.128132Z","iopub.status.idle":"2025-10-25T14:01:47.244817Z","shell.execute_reply.started":"2025-10-25T14:01:47.128101Z","shell.execute_reply":"2025-10-25T14:01:47.243935Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\"\"\"\nData Augmentation using examples in train\n\"\"\"\n\n# positives\npos = df_train[[\"positive_example_1\", \"rule\", \"subreddit\"]].rename(\n    columns={\"positive_example_1\": \"body\"}\n)\npos[\"rule_violation\"] = 1\n\npos_2 = df_train[[\"positive_example_2\", \"rule\", \"subreddit\"]].rename(\n    columns={\"positive_example_2\": \"body\"}\n)\npos_2[\"rule_violation\"] = 1\n\n# negatives\nneg = df_train[[\"negative_example_1\", \"rule\", \"subreddit\"]].rename(\n    columns={\"negative_example_1\": \"body\"}\n)\nneg[\"rule_violation\"] = 0\n\nneg_2 = df_train[[\"negative_example_2\", \"rule\", \"subreddit\"]].rename(\n    columns={\"negative_example_2\": \"body\"}\n)\nneg_2[\"rule_violation\"] = 0\n\n# combine\ndf_add = pd.concat([pos, pos_2, neg, neg_2], ignore_index=True)\n\n# optional: drop missing texts, ensure int dtype\ndf_add = df_add.dropna(subset=[\"body\"]).reset_index(drop=True)\ndf_add[\"rule_violation\"] = df_add[\"rule_violation\"].astype(int)\ndf_train = pd.concat([df_train, df_add], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:47.245671Z","iopub.execute_input":"2025-10-25T14:01:47.245921Z","iopub.status.idle":"2025-10-25T14:01:47.277833Z","shell.execute_reply.started":"2025-10-25T14:01:47.245900Z","shell.execute_reply":"2025-10-25T14:01:47.276898Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\"\"\"\nData Augmentation using examples in test\n\"\"\"\n\n# positives\npos = df_test[[\"positive_example_1\", \"rule\", \"subreddit\"]].rename(\n    columns={\"positive_example_1\": \"body\"}\n)\npos[\"rule_violation\"] = 1\n\npos_2 = df_test[[\"positive_example_2\", \"rule\", \"subreddit\"]].rename(\n    columns={\"positive_example_2\": \"body\"}\n)\npos_2[\"rule_violation\"] = 1\n\n# negatives\nneg = df_test[[\"negative_example_1\", \"rule\", \"subreddit\"]].rename(\n    columns={\"negative_example_1\": \"body\"}\n)\nneg[\"rule_violation\"] = 0\n\nneg_2 = df_test[[\"negative_example_2\", \"rule\", \"subreddit\"]].rename(\n    columns={\"negative_example_2\": \"body\"}\n)\nneg_2[\"rule_violation\"] = 0\n\n# combine\ndf_add = pd.concat([pos, pos_2, neg, neg_2], ignore_index=True)\n\n# optional: drop missing texts, ensure int dtype\ndf_add = df_add.dropna(subset=[\"body\"]).reset_index(drop=True)\ndf_add[\"rule_violation\"] = df_add[\"rule_violation\"].astype(int)\ndf_train_aug = pd.concat([df_train, df_add], ignore_index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:47.280394Z","iopub.execute_input":"2025-10-25T14:01:47.280698Z","iopub.status.idle":"2025-10-25T14:01:47.300093Z","shell.execute_reply.started":"2025-10-25T14:01:47.280665Z","shell.execute_reply":"2025-10-25T14:01:47.299429Z"}},"outputs":[],"execution_count":5},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"code","source":"%%writefile utils_bert.py\n\nimport urllib.request\nimport re\nfrom urllib.parse import urlparse\nimport emoji\nimport pandas as pd\n\n\ndef add_rule_and_subreddit(df, is_train = True):\n    new_df = pd.DataFrame()\n    new_df[\"data\"] = (\n        \"Rule: \" + df[\"rule\"] + \" [SEP] \" +\n        \"Subreddit: \" + df[\"subreddit\"] + \" [SEP] \" +\n        \"Comment: \" + df[\"body\"]\n    )\n    if is_train:\n        new_df[\"label\"] = df[\"rule_violation\"]\n        \n    return new_df\n\n\ndef replace_urls_with_features(text):\n    urls = re.findall(r\"(?:http|https)://[^\\s]+\", text)\n\n    for url in urls:\n        seen_semantics = set()\n        all_semantics = []    \n        try:\n            parsed = urlparse(url)\n            domain = parsed.netloc.lower()\n        except ValueError:\n            domain = \"invalid\"\n\n        # domain\n        domain_match = re.search(r\"(?:https?://)?([a-z0-9\\-\\.]+)\\.[a-z]{2,}\", url.lower())\n        if domain_match:\n            full_domain = domain_match.group(1)\n            parts = full_domain.split('.')\n            for part in parts:\n                if part and part not in seen_semantics and len(part) > 3:\n                    all_semantics.append(f\"domain:{part}\")\n                    seen_semantics.add(part)\n\n        # path\n        path = re.sub(r\"^(?:https?://)?[a-z0-9\\.-]+\\.[a-z]{2,}/?\", \"\", url.lower())\n        path_parts = [p for p in re.split(r'[/_.-]+', path) if p and p.isalnum()]\n        for part in path_parts:\n            part_clean = re.sub(r\"\\.(html?|php|asp|jsp)$|#.*|\\?.*\", \"\", part)\n            if part_clean and part_clean not in seen_semantics and len(part_clean) > 3:\n                all_semantics.append(f\"path:{part_clean}\")\n                seen_semantics.add(part_clean)\n\n        if all_semantics:\n            semantic_str = f\"\\n(URL Keywords: {' '.join(all_semantics)})\"\n        else:\n            semantic_str = \"\"\n\n        text = text.replace(url, semantic_str)\n\n    return text\n\n\ndef clean_text(text):\n    text = replace_urls_with_features(text) # extract semantics from URL\n    text = emoji.replace_emoji(text, replace=\"\")  # remove emoji\n    text = re.sub(r'\\s+', ' ', text).strip() # remove unnecessary space\n    return text\n\n\ndef data_processing(df, is_train = True):\n    df = add_rule_and_subreddit(df, is_train)\n    df[\"data\"] = df['data'].apply(clean_text)\n    \n    if is_train:\n        df = df.drop_duplicates()\n\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:31:15.146658Z","iopub.execute_input":"2025-10-25T14:31:15.147357Z","iopub.status.idle":"2025-10-25T14:31:15.153925Z","shell.execute_reply.started":"2025-10-25T14:31:15.147331Z","shell.execute_reply":"2025-10-25T14:31:15.153043Z"}},"outputs":[{"name":"stdout","text":"Writing utils_bert.py\n","output_type":"stream"}],"execution_count":27},{"cell_type":"code","source":"\"\"\"\nData Processing\n\"\"\"\nfrom utils_bert import data_processing\n\ndf_train_aug = data_processing(df_train_aug, is_train=True)\ndf_train_aug = df_train_aug.sample(frac=1).reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:47.322799Z","iopub.execute_input":"2025-10-25T14:01:47.323057Z","iopub.status.idle":"2025-10-25T14:01:49.912039Z","shell.execute_reply.started":"2025-10-25T14:01:47.323033Z","shell.execute_reply":"2025-10-25T14:01:49.911408Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\"\"\"\nDevide train data into 3 \n\"\"\"\nsize = len(df_train_aug)\ni1 = (size * 1) // 5\ni2 = (size * 2) // 5\ni3 = (size * 3) // 5\n\nprint(f\"data size {size}\")\nprint(f\"split data into 0-{i1}, {i1}-{i2}, {i2}-{i3}\")\n\ndf_train_aug_1 = df_train_aug.iloc[0:i1].copy()\ndf_train_aug_2 = df_train_aug.iloc[i1:i2].copy()\ndf_train_aug_3 = df_train_aug.iloc[i2:i3].copy()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:49.912890Z","iopub.execute_input":"2025-10-25T14:01:49.913175Z","iopub.status.idle":"2025-10-25T14:01:49.919834Z","shell.execute_reply.started":"2025-10-25T14:01:49.913152Z","shell.execute_reply":"2025-10-25T14:01:49.919085Z"}},"outputs":[{"name":"stdout","text":"data size 7913\nsplit data into 0-1582, 1582-3165, 3165-4747\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"\"\"\"\nSave train data\n\"\"\"\ndf_train_aug_1.to_csv('/kaggle/working/train_data_1.csv', index=False)  \ndf_train_aug_2.to_csv('/kaggle/working/train_data_2.csv', index=False)  \ndf_train_aug_3.to_csv('/kaggle/working/train_data_3.csv', index=False) ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:49.920582Z","iopub.execute_input":"2025-10-25T14:01:49.921008Z","iopub.status.idle":"2025-10-25T14:01:49.991202Z","shell.execute_reply.started":"2025-10-25T14:01:49.920988Z","shell.execute_reply":"2025-10-25T14:01:49.990595Z"}},"outputs":[],"execution_count":9},{"cell_type":"markdown","source":"# Fine Tuning","metadata":{}},{"cell_type":"markdown","source":"## (1) Bert-base","metadata":{}},{"cell_type":"code","source":"# %%writefile train_bert.py\n\n# import argparse, pandas as pd\n# from constants import BATCH_SIZE, MAX_TOKEN, BERT_EPOCH, BERT_MODEL_DIR, OUT_DIR, SEED\n# import torch\n# import torch.optim as optim\n# import torch.nn as nn\n# from torch.utils.data import DataLoader, TensorDataset\n# from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n# from sklearn.metrics import accuracy_score, precision_score, recall_score\n# from sklearn.model_selection import train_test_split\n# import copy\n# from sklearn.metrics import accuracy_score, f1_score\n\n# device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n\n# # Token and Encode Function\n# def tokenize_and_encode(tokenizer, comments, labels, max_length):\n#     # Initialize empty lists to store tokenized inputs and attention masks\n#     input_ids = []\n#     attention_masks = []\n\n#     # Iterate through each comment in the 'comments' list\n#     for comment in comments:\n\n#         # Tokenize and encode the comment using the BERT tokenizer\n#         encoded_dict = tokenizer.encode_plus(\n#             comment,\n\n#             # Add special tokens like [CLS] and [SEP]\n#             add_special_tokens=True,\n\n#             truncation=True,\n            \n#             # Truncate or pad the comment to 'max_length'\n#             max_length=max_length,\n\n#             # Pad the comment to 'max_length' with zeros if needed\n#             padding='max_length',\n\n#             # Return attention mask to mask padded tokens\n#             return_attention_mask=True,\n\n#             # Return PyTorch tensors\n#             return_tensors='pt'\n#         )\n\n#         # Append the tokenized input and attention mask to their respective lists\n#         input_ids.append(encoded_dict['input_ids'])\n#         attention_masks.append(encoded_dict['attention_mask'])\n\n#     # Concatenate the tokenized inputs and attention masks into tensors\n#     input_ids = torch.cat(input_ids, dim=0)\n#     attention_masks = torch.cat(attention_masks, dim=0)\n\n#     # Convert the labels to a PyTorch tensor with the data type float32\n#     labels = torch.tensor(labels, dtype=torch.float32)\n\n#     # Return the tokenized inputs, attention masks, and labels as PyTorch tensors\n#     return input_ids, attention_masks, labels\n\n    \n# def train_model(model, train_loader, val_loader, device, num_epochs, patience=2):\n#     # loss_fn = nn.BCELoss()  # binary cross entropy\n#     loss_fn = nn.BCEWithLogitsLoss()\n#     optimizer = optim.Adam(model.parameters(), lr=2e-5)\n\n#     # loss_fn = nn.BCEWithLogitsLoss()  \n#     # optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n    \n#     best_f1 = -1.0\n#     # best_auc = -1.0\n#     epochs_no_improve = 0\n#     best_state = None\n\n#     train_losses, val_losses, train_f1s, val_f1s = [], [], [], []\n#     # train_losses, val_losses, train_aucs, val_aucs = [], [], [], []\n    \n#     for epoch in range(num_epochs):\n\n#         model.train()\n#         total_loss = 0.0\n#         all_train_preds, all_train_labels = [], []\n#         # train\n#         for batch in train_loader:\n            \n#             input_ids, attention_mask, labels = [t.to(device) for t in batch]\n#             labels = labels.float()\n\n#             # prediction (number of batches)\n#             output = model(input_ids=input_ids, attention_mask=attention_mask)\n#             logits = output.logits.squeeze(-1)\n#             probs = torch.sigmoid(logits)  # convert logits to probabilities first\n#             # print(logits) # DEBUG\n                 \n#             # forward pass\n#             # loss = loss_fn(probs, labels)\n#             loss = loss_fn(logits, labels)\n#             # print(loss) # DEBUG\n            \n#             total_loss += loss.item()\n\n#             # backward pass\n#             optimizer.zero_grad()\n#             loss.backward()\n            \n#             # update weights\n#             optimizer.step()\n\n#             preds = (probs > 0.5).long()\n#             all_train_preds.extend(preds.cpu().tolist())\n#             all_train_labels.extend(labels.cpu().tolist())\n            \n#         train_loss = total_loss / len(train_loader)\n#         train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n\n#         train_losses.append(train_loss)\n#         train_f1s.append(train_f1)\n        \n#         # Validate\n#         model.eval()\n#         val_loss = 0.0\n#         all_preds, all_labels = [], []\n#         with torch.no_grad():\n#             for batch in val_loader:\n                \n#                 input_ids, attention_mask, labels = [t.to(device) for t in batch]\n#                 labels = labels.float()\n#                 output = model(input_ids=input_ids, attention_mask=attention_mask)\n#                 logits = output.logits.squeeze(-1)\n#                 probs = torch.sigmoid(logits)\n                \n#                 # val_loss += loss_fn(probs, labels.float()).item()\n#                 val_loss += loss_fn(logits, labels.float()).item()\n                \n#                 preds = (probs > 0.5).long()\n#                 all_preds.extend(preds.cpu().tolist())\n#                 all_labels.extend(labels.cpu().tolist())\n                \n#         val_f1 = f1_score(all_labels, all_preds, average='macro')\n#         val_f1s.append(val_f1)\n        \n#         val_losses.append(val_loss)\n#         val_loss /= len(val_loader)\n        \n#         print(f\"Epoch {epoch+1}/{num_epochs} - \"\n#               f\"Train Loss: {train_loss:.4f}, Train Acc: {train_f1:.4f} - \"\n#               f\"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n\n#         if val_f1 > best_f1:\n#             best_f1 = val_f1\n#             epochs_no_improve = 0\n#             best_state = copy.deepcopy(model.state_dict())\n#         else:\n#             epochs_no_improve += 1\n#             if epochs_no_improve >= patience:\n#                 print(f\"Early stopping (no val F1 improvement for {patience} epochs). Best Val F1: {best_f1:.4f}\")\n#                 break\n\n#     if best_state is not None:\n#         model.load_state_dict(best_state)\n\n#     return model\n\n\n# def main():\n#     parser = argparse.ArgumentParser()\n#     parser.add_argument(\"--data\", type=str, required=True)\n#     args = parser.parse_args()\n\n#     df_train_aug_1 = pd.read_csv(args.data)\n#     y = df_train_aug_1[\"label\"]\n#     X = df_train_aug_1[\"data\"]\n#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n\n#     tokenizer = BertTokenizer.from_pretrained(\n#         BERT_MODEL_DIR,\n#         do_lower_case=True,\n#         local_files_only=True\n#     )\n    \n#     config = BertConfig.from_pretrained(\n#         BERT_MODEL_DIR,\n#         local_files_only=True\n#     )\n    \n#     model = BertForSequenceClassification.from_pretrained(\n#         BERT_MODEL_DIR,\n#         config=config,\n#         local_files_only=True\n#     )\n#     model = model.to(device)\n    \n#     input_ids, attention_masks, labels = tokenize_and_encode(\n#         tokenizer,\n#         X_train,\n#         y_train.values,\n#         MAX_TOKEN,\n#     )\n\n#     val_input_ids, val_attention_masks, val_labels = tokenize_and_encode(\n#         tokenizer,\n#         X_val,\n#         y_val.values,\n#         MAX_TOKEN,\n#     )\n\n#     # Creating DataLoader for the balanced dataset\n#     train_dataset = TensorDataset(input_ids, attention_masks, labels)\n#     train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    \n#     # validation set \n#     val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n#     val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n#     model = train_model(model, train_loader, val_loader, device, num_epochs=BERT_EPOCH)\n#     torch.save(model.state_dict(), OUT_DIR+\"bert\")\n\n# if __name__ == \"__main__\":\n#     main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:49.991994Z","iopub.execute_input":"2025-10-25T14:01:49.992210Z","iopub.status.idle":"2025-10-25T14:01:49.999155Z","shell.execute_reply.started":"2025-10-25T14:01:49.992193Z","shell.execute_reply":"2025-10-25T14:01:49.998233Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# !python train_bert.py --data /kaggle/working/train_data_1.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:49.999993Z","iopub.execute_input":"2025-10-25T14:01:50.000688Z","iopub.status.idle":"2025-10-25T14:01:50.013895Z","shell.execute_reply.started":"2025-10-25T14:01:50.000662Z","shell.execute_reply":"2025-10-25T14:01:50.013240Z"}},"outputs":[],"execution_count":11},{"cell_type":"markdown","source":"## (2) RoBERTa","metadata":{}},{"cell_type":"code","source":"%%writefile train_roberta.py\n\nimport argparse, pandas as pd\nfrom constants import BATCH_SIZE, MAX_TOKEN, ROBERTA_EPOCH, ROBERTA_MODEL_DIR, OUT_DIR, SEED\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import (\n    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n    DataCollatorWithPadding\n)\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score\nfrom sklearn.model_selection import train_test_split\nimport copy\nfrom sklearn.metrics import accuracy_score, f1_score\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\n\n# Token and Encode Function\ndef tokenize_and_encode(tokenizer, comments, labels, max_length):\n    # Initialize empty lists to store tokenized inputs and attention masks\n    input_ids = []\n    attention_masks = []\n\n    # Iterate through each comment in the 'comments' list\n    for comment in comments:\n\n        # Tokenize and encode the comment using the BERT tokenizer\n        encoded_dict = tokenizer.encode_plus(\n            comment,\n\n            # Add special tokens like [CLS] and [SEP]\n            add_special_tokens=True,\n\n            truncation=True,\n            \n            # Truncate or pad the comment to 'max_length'\n            max_length=max_length,\n\n            # Pad the comment to 'max_length' with zeros if needed\n            padding='max_length',\n\n            # Return attention mask to mask padded tokens\n            return_attention_mask=True,\n\n            # Return PyTorch tensors\n            return_tensors='pt'\n        )\n\n        # Append the tokenized input and attention mask to their respective lists\n        input_ids.append(encoded_dict['input_ids'])\n        attention_masks.append(encoded_dict['attention_mask'])\n\n    # Concatenate the tokenized inputs and attention masks into tensors\n    input_ids = torch.cat(input_ids, dim=0)\n    attention_masks = torch.cat(attention_masks, dim=0)\n\n    # Convert the labels to a PyTorch tensor with the data type float32\n    labels = torch.tensor(labels, dtype=torch.float32)\n\n    # Return the tokenized inputs, attention masks, and labels as PyTorch tensors\n    return input_ids, attention_masks, labels\n\n    \ndef train_model(model, train_loader, val_loader, device, num_epochs, patience=2):\n    # loss_fn = nn.BCELoss()  # binary cross entropy\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=2e-5, weight_decay=0.01) # weight_decay=0.01\n\n    # loss_fn = nn.BCEWithLogitsLoss()  \n    # optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n    \n    best_f1 = -1.0\n    # best_auc = -1.0\n    epochs_no_improve = 0\n    best_state = None\n\n    train_losses, val_losses, train_f1s, val_f1s = [], [], [], []\n    # train_losses, val_losses, train_aucs, val_aucs = [], [], [], []\n    \n    for epoch in range(num_epochs):\n\n        model.train()\n        total_loss = 0.0\n        all_train_preds, all_train_labels = [], []\n        # train\n        for batch in train_loader:\n            \n            input_ids, attention_mask, labels = [t.to(device) for t in batch]\n            labels = labels.float()\n\n            # prediction (number of batches)\n            output = model(input_ids=input_ids, attention_mask=attention_mask)\n            logits = output.logits.squeeze(-1)\n            probs = torch.sigmoid(logits)  # convert logits to probabilities first\n            # print(logits) # DEBUG\n                 \n            # forward pass\n            # loss = loss_fn(probs, labels)\n            loss = loss_fn(logits, labels)\n            # print(loss) # DEBUG\n            \n            total_loss += loss.item()\n\n            # backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            \n            # update weights\n            optimizer.step()\n\n            preds = (probs > 0.5).long()\n            all_train_preds.extend(preds.cpu().tolist())\n            all_train_labels.extend(labels.cpu().tolist())\n            \n        train_loss = total_loss / len(train_loader)\n        train_f1 = f1_score(all_train_labels, all_train_preds, average='macro')\n\n        train_losses.append(train_loss)\n        train_f1s.append(train_f1)\n        \n        # Validate\n        model.eval()\n        val_loss = 0.0\n        all_preds, all_labels = [], []\n        with torch.no_grad():\n            for batch in val_loader:\n                \n                input_ids, attention_mask, labels = [t.to(device) for t in batch]\n                labels = labels.float()\n                output = model(input_ids=input_ids, attention_mask=attention_mask)\n                logits = output.logits.squeeze(-1)\n                probs = torch.sigmoid(logits)\n                \n                # val_loss += loss_fn(probs, labels.float()).item()\n                val_loss += loss_fn(logits, labels.float()).item()\n                \n                preds = (probs > 0.5).long()\n                all_preds.extend(preds.cpu().tolist())\n                all_labels.extend(labels.cpu().tolist())\n                \n        val_f1 = f1_score(all_labels, all_preds, average='macro')\n        val_f1s.append(val_f1)\n        \n        val_losses.append(val_loss)\n        val_loss /= len(val_loader)\n        \n        print(f\"Epoch {epoch+1}/{num_epochs} - \"\n              f\"Train Loss: {train_loss:.4f}, Train Acc: {train_f1:.4f} - \"\n              f\"Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}\")\n\n        if val_f1 > best_f1:\n            best_f1 = val_f1\n            epochs_no_improve = 0\n            best_state = copy.deepcopy(model.state_dict())\n        else:\n            epochs_no_improve += 1\n            if epochs_no_improve >= patience:\n                print(f\"Early stopping (no val F1 improvement for {patience} epochs). Best Val F1: {best_f1:.4f}\")\n                break\n\n    if best_state is not None:\n        model.load_state_dict(best_state)\n\n    return model\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--data\", type=str, required=True)\n    parser.add_argument(\"--num\", type=str, required=True)\n\n    args = parser.parse_args()\n\n    df_train_aug_1 = pd.read_csv(args.data)\n    y = df_train_aug_1[\"label\"]\n    X = df_train_aug_1[\"data\"]\n    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=SEED)\n\n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        ROBERTA_MODEL_DIR,\n        use_fast=True,\n        local_files_only=True\n    )  \n    \n    # Config\n    config = AutoConfig.from_pretrained(\n        ROBERTA_MODEL_DIR,\n        num_labels=1,  # 1 --> regression (MSE Loss), 2--> binary classification (Cross-entropy)\n        problem_type=\"single_label_classification\",\n        local_files_only=True,\n        hidden_dropout_prob=0.2,\n        attention_probs_dropout_prob=0.2,\n    )\n    \n    # Model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        ROBERTA_MODEL_DIR,\n        config=config,\n        local_files_only=True\n    )\n    model = model.to(device)\n    \n    input_ids, attention_masks, labels = tokenize_and_encode(\n        tokenizer,\n        X_train,\n        y_train.values,\n        MAX_TOKEN,\n    )\n\n    val_input_ids, val_attention_masks, val_labels = tokenize_and_encode(\n        tokenizer,\n        X_val,\n        y_val.values,\n        MAX_TOKEN,\n    )\n\n    # Creating DataLoader for the balanced dataset\n    train_dataset = TensorDataset(input_ids, attention_masks, labels)\n    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n    \n    # validation set \n    val_dataset = TensorDataset(val_input_ids, val_attention_masks, val_labels)\n    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n    \n    model = train_model(model, train_loader, val_loader, device, num_epochs=ROBERTA_EPOCH)\n    torch.save(model.state_dict(), OUT_DIR+\"roberta_\"+args.num)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:50.014697Z","iopub.execute_input":"2025-10-25T14:01:50.015177Z","iopub.status.idle":"2025-10-25T14:01:50.029679Z","shell.execute_reply.started":"2025-10-25T14:01:50.015151Z","shell.execute_reply":"2025-10-25T14:01:50.028946Z"}},"outputs":[{"name":"stdout","text":"Writing train_roberta.py\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"!python train_roberta.py --data /kaggle/working/train_data_1.csv --num 1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:01:50.030667Z","iopub.execute_input":"2025-10-25T14:01:50.030928Z","iopub.status.idle":"2025-10-25T14:11:41.530849Z","shell.execute_reply.started":"2025-10-25T14:01:50.030910Z","shell.execute_reply":"2025-10-25T14:11:41.529993Z"}},"outputs":[{"name":"stdout","text":"2025-10-25 14:02:04.144333: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761400924.378776      84 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761400924.433437      84 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/9 - Train Loss: 0.6908, Train Acc: 0.4756 - Val Loss: 0.6935, Val F1: 0.3241\nEpoch 2/9 - Train Loss: 0.6880, Train Acc: 0.5172 - Val Loss: 0.6919, Val F1: 0.3241\nEpoch 3/9 - Train Loss: 0.6792, Train Acc: 0.5689 - Val Loss: 0.6465, Val F1: 0.5480\nEpoch 4/9 - Train Loss: 0.5600, Train Acc: 0.7092 - Val Loss: 0.4352, Val F1: 0.7981\nEpoch 5/9 - Train Loss: 0.4046, Train Acc: 0.8257 - Val Loss: 0.2888, Val F1: 0.8984\nEpoch 6/9 - Train Loss: 0.2530, Train Acc: 0.9040 - Val Loss: 0.1966, Val F1: 0.9335\nEpoch 7/9 - Train Loss: 0.1723, Train Acc: 0.9422 - Val Loss: 0.1891, Val F1: 0.9431\nEpoch 8/9 - Train Loss: 0.1211, Train Acc: 0.9580 - Val Loss: 0.2342, Val F1: 0.9305\nEpoch 9/9 - Train Loss: 0.1275, Train Acc: 0.9565 - Val Loss: 0.2150, Val F1: 0.9303\nEarly stopping (no val F1 improvement for 2 epochs). Best Val F1: 0.9431\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"!python train_roberta.py --data /kaggle/working/train_data_2.csv --num 2","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:11:41.533862Z","iopub.execute_input":"2025-10-25T14:11:41.534168Z","iopub.status.idle":"2025-10-25T14:21:19.403310Z","shell.execute_reply.started":"2025-10-25T14:11:41.534146Z","shell.execute_reply":"2025-10-25T14:21:19.402275Z"}},"outputs":[{"name":"stdout","text":"2025-10-25 14:11:46.975136: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761401506.998390     106 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761401507.005451     106 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nEpoch 1/9 - Train Loss: 0.6914, Train Acc: 0.4918 - Val Loss: 0.6854, Val F1: 0.3544\nEpoch 2/9 - Train Loss: 0.6901, Train Acc: 0.4915 - Val Loss: 0.6809, Val F1: 0.3918\nEpoch 3/9 - Train Loss: 0.6774, Train Acc: 0.5664 - Val Loss: 0.6272, Val F1: 0.6907\nEpoch 4/9 - Train Loss: 0.5374, Train Acc: 0.7306 - Val Loss: 0.3311, Val F1: 0.8686\nEpoch 5/9 - Train Loss: 0.3203, Train Acc: 0.8752 - Val Loss: 0.2768, Val F1: 0.8883\nEpoch 6/9 - Train Loss: 0.3229, Train Acc: 0.8655 - Val Loss: 0.2593, Val F1: 0.9110\nEpoch 7/9 - Train Loss: 0.1810, Train Acc: 0.9415 - Val Loss: 0.2269, Val F1: 0.9204\nEpoch 8/9 - Train Loss: 0.1399, Train Acc: 0.9462 - Val Loss: 0.2349, Val F1: 0.9232\nEpoch 9/9 - Train Loss: 0.1159, Train Acc: 0.9628 - Val Loss: 0.2381, Val F1: 0.9299\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## (3) deberta-v3-base","metadata":{}},{"cell_type":"markdown","source":"Credit: https://www.kaggle.com/code/jonathanchan/jigsaw25-dbv3b-seed-ensemble","metadata":{}},{"cell_type":"code","source":"%%writefile utils_deberta.py\nimport pandas as pd\nimport re\nfrom typing import List\n\n# Pre-compiled regex patterns\nURL_PATTERN = re.compile(r'https?://[^\\s/$.?#].[^\\s]*')\nDOMAIN_PATTERN = re.compile(r\"(?:https?://)?([a-z0-9\\-\\.]+)\\.[a-z]{2,}\")\nCLEAN_PATH_PART_PATTERN = re.compile(r\"\\.(html?|php|asp|jsp)$|#.*|\\?.*\")\n\n\ndef url_to_semantics(text: str) -> str:\n    \"\"\"\n    Extracts semantic keywords from URLs found in a given text.\n    Returns a formatted string containing semantic features.\n    \"\"\"\n    if not isinstance(text, str):\n        return \"\"\n\n    urls = URL_PATTERN.findall(text)\n    if not urls:\n        return \"\" \n\n    all_semantics = []\n    seen_semantics = set()\n\n    for url in urls:\n        url_lower = url.lower()\n\n        # Extract domain parts\n        domain_match = DOMAIN_PATTERN.search(url_lower)\n        if domain_match:\n            full_domain = domain_match.group(1)\n            for part in full_domain.split('.'):\n                if part and part not in seen_semantics and len(part) > 3:\n                    all_semantics.append(f\"domain:{part}\")\n                    seen_semantics.add(part)\n\n        # Extract path parts\n        path = re.sub(r\"^(?:https?://)?[a-z0-9\\.-]+\\.[a-z]{2,}/?\", \"\", url_lower)\n        path_parts = [p for p in re.split(r'[/_.-]+', path) if p and p.isalnum()]\n        \n        for part in path_parts:\n            part_clean = CLEAN_PATH_PART_PATTERN.sub(\"\", part)\n            if part_clean and part_clean not in seen_semantics and len(part_clean) > 3:\n                all_semantics.append(f\"path:{part_clean}\")\n                seen_semantics.add(part_clean)\n\n    return f\"\\nURL Keywords: {' '.join(all_semantics)}\" if all_semantics else \"\"\n\n\ndef extract_violation_examples(df: pd.DataFrame, dataset_type: str) -> List[pd.DataFrame]:\n    \"\"\"\n    Extracts positive and negative examples from a given dataset.\n    Returns a list of cleaned and labeled DataFrames.\n    \"\"\"\n    examples = []\n    for violation_type in [\"positive\", \"negative\"]:\n        label = 1 if violation_type == \"positive\" else 0\n        for i in range(1, 3):\n            col = f\"{violation_type}_example_{i}\"\n            if col in df.columns:\n                sub_df = df[[col, \"rule\", \"subreddit\"]].copy()\n                sub_df = sub_df.rename(columns={col: \"body\"})\n                sub_df[\"rule_violation\"] = label\n                sub_df.dropna(subset=[\"body\"], inplace=True)\n                sub_df = sub_df[sub_df[\"body\"].str.strip().str.len() > 0]\n                if not sub_df.empty:\n                    examples.append(sub_df)\n    return examples\n\n\ndef get_dataframe_to_train(data_path: str, seed: int = 42) -> pd.DataFrame:\n    \"\"\"\n    Loads train and test datasets, extracts and flattens rule violation examples,\n    deduplicates, and returns a shuffled DataFrame ready for training.\n\n    Parameters:\n        data_path (str): Path to the dataset.\n        seed (int): Random seed for reproducible shuffling.\n\n    Returns:\n        pd.DataFrame: Cleaned and shuffled training dataset.\n    \"\"\"\n    train_df = pd.read_csv(f\"{data_path}/train.csv\")\n    test_df = pd.read_csv(f\"{data_path}/test.csv\")\n\n    combined = []\n\n    if {\"body\", \"rule\", \"subreddit\", \"rule_violation\"}.issubset(train_df.columns):\n        combined.append(train_df[[\"body\", \"rule\", \"subreddit\", \"rule_violation\"]].copy())\n\n    combined.extend(extract_violation_examples(train_df, \"train\"))\n    combined.extend(extract_violation_examples(test_df, \"test\"))\n\n    full_df = pd.concat(combined, axis=0, ignore_index=True)\n\n    # Deduplicate\n    full_df.drop_duplicates(subset=[\"body\", \"rule\", \"subreddit\"], inplace=True)\n    full_df.drop_duplicates(subset=[\"body\", \"rule\"], keep=\"first\", inplace=True)\n\n    # Shuffle with specified seed\n    return full_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:33:48.638964Z","iopub.execute_input":"2025-10-25T14:33:48.639258Z","iopub.status.idle":"2025-10-25T14:33:48.646601Z","shell.execute_reply.started":"2025-10-25T14:33:48.639236Z","shell.execute_reply":"2025-10-25T14:33:48.645893Z"}},"outputs":[{"name":"stdout","text":"Writing utils_deberta.py\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"%%writefile train_deberta.py\nimport os\nimport pandas as pd\nimport torch\nimport random\nimport numpy as np\nimport argparse\nfrom sklearn.model_selection import train_test_split \nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments\n)\n\nfrom utils_deberta import get_dataframe_to_train, url_to_semantics\n\ndef seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed) \n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nclass CFG:\n    model_name_or_path = \"/kaggle/input/debertav3base\"\n#    model_name_or_path = \"/kaggle/input/deberta-v3-small/deberta-v3-small\"\n    data_path = \"/kaggle/input/jigsaw-agile-community-rules/\"\n    output_dir = \"./dbv3_base_ens_model\"\n#    output_dir = \"./dbv3_small_ens_model\"\n  \n    EPOCHS = 5\n    LEARNING_RATE = 2e-5  \n    \n    MAX_LENGTH = 246\n    BATCH_SIZE = 8\n\nclass JigsawDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels=None):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        if self.labels:\n            item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.encodings['input_ids'])\n\ndef add_url_semantics_column(df):\n    return df['body'].apply(lambda x: x + url_to_semantics(x))\n\ndef build_input_text(df):\n    return df['rule'] + \"[SEP]\" + df['body_with_url']\n\ndef prepare_dataset(df, tokenizer, max_length, is_train=True):\n    encodings = tokenizer(\n        df['input_text'].tolist(),\n        truncation=True,\n        padding=True,\n        max_length=max_length\n    )\n    if is_train:\n        labels = df['rule_violation'].tolist()\n        return JigsawDataset(encodings, labels)\n    else:\n        return JigsawDataset(encodings)\n\ndef train(seed=42):\n    seed_everything(seed)\n    print(f\"\\n Loading and preparing training data with seed={seed} ...\")\n    training_data_df = get_dataframe_to_train(CFG.data_path, seed=seed)\n    print(f\"Training dataset size: {len(training_data_df)}\")\n\n    training_data_df['body_with_url'] = add_url_semantics_column(training_data_df)\n    training_data_df['input_text'] = build_input_text(training_data_df)\n\n    tokenizer = AutoTokenizer.from_pretrained(CFG.model_name_or_path)\n    train_dataset = prepare_dataset(training_data_df, tokenizer, CFG.MAX_LENGTH, is_train=True)\n\n    model = AutoModelForSequenceClassification.from_pretrained(CFG.model_name_or_path, num_labels=2)\n\n    training_args = TrainingArguments(\n        output_dir=CFG.output_dir,\n        num_train_epochs=CFG.EPOCHS,\n        learning_rate=CFG.LEARNING_RATE,\n        per_device_train_batch_size=CFG.BATCH_SIZE,\n        warmup_ratio=0.1,\n        weight_decay=0.01,\n        report_to=\"none\",\n        save_strategy=\"no\",\n        logging_steps=1,\n        fp16=True,\n    )\n\n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n    )\n\n    print(\"Starting training...\")\n    trainer.train()\n    print(\"Training complete.\")\n\n    return trainer, tokenizer\n\ndef predict(trainer, tokenizer, seed):\n    print(f\"\\nLoading test data for seed={seed}...\")\n    test_df = pd.read_csv(f\"{CFG.data_path}/test.csv\")\n    test_df['body_with_url'] = add_url_semantics_column(test_df)\n    test_df['input_text'] = build_input_text(test_df)\n    \n    test_dataset = prepare_dataset(test_df, tokenizer, CFG.MAX_LENGTH, is_train=False)\n\n    print(\"Running prediction...\")\n    predictions = trainer.predict(test_dataset)\n    probs = torch.nn.functional.softmax(torch.tensor(predictions.predictions), dim=1)[:, 1].numpy()\n\n    submission_df = pd.DataFrame({\n        \"row_id\": test_df[\"row_id\"],\n        \"rule_violation\": probs\n    })\n\n    output_file = f\"submission_deberta_{seed}.csv\"\n    submission_df.to_csv(output_file, index=False)\n    print(f\"ðŸ“¤ Saved submission to: {output_file}\")\n\ndef main():\n    parser = argparse.ArgumentParser(description=\"Train DeBERTa on rule violation data with multiple seeds.\")\n    parser.add_argument(\n        \"--seeds\",\n        type=str,\n        default=\"42\",\n        help=\"Comma-separated list of seeds to run (e.g. 1,42,1337)\"\n    )\n    args = parser.parse_args()\n\n    seed_list = [int(s) for s in args.seeds.split(\",\") if s.strip().isdigit()]\n    \n    for seed in seed_list:\n        print(f\"\\n\\n===== Running training and prediction for seed: {seed} =====\")\n        trainer, tokenizer = train(seed=seed)\n        predict(trainer, tokenizer, seed=seed)\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:33:52.713941Z","iopub.execute_input":"2025-10-25T14:33:52.714685Z","iopub.status.idle":"2025-10-25T14:33:52.721714Z","shell.execute_reply.started":"2025-10-25T14:33:52.714658Z","shell.execute_reply":"2025-10-25T14:33:52.721094Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_deberta.py\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"!python train_deberta.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:34:01.189329Z","iopub.execute_input":"2025-10-25T14:34:01.189921Z","iopub.status.idle":"2025-10-25T14:40:58.244748Z","shell.execute_reply.started":"2025-10-25T14:34:01.189899Z","shell.execute_reply":"2025-10-25T14:40:58.243847Z"}},"outputs":[{"name":"stdout","text":"2025-10-25 14:34:07.565017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761402847.588077     190 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761402847.595573     190 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\n\n===== Running training and prediction for seed: 42 =====\n\n Loading and preparing training data with seed=42 ...\nTraining dataset size: 1875\n/usr/local/lib/python3.11/dist-packages/transformers/convert_slow_tokenizer.py:564: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n  warnings.warn(\nSome weights of DebertaV2ForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/debertav3base and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.bias', 'pooler.dense.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nStarting training...\n  0%|                                                   | 0/590 [00:00<?, ?it/s]/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n{'loss': 0.7033, 'grad_norm': 78737.5546875, 'learning_rate': 0.0, 'epoch': 0.01}\n{'loss': 0.7259, 'grad_norm': 140314.140625, 'learning_rate': 3.3898305084745766e-07, 'epoch': 0.02}\n{'loss': 0.6992, 'grad_norm': 49431.47265625, 'learning_rate': 6.779661016949153e-07, 'epoch': 0.03}\n{'loss': 0.6695, 'grad_norm': 68161.671875, 'learning_rate': 1.016949152542373e-06, 'epoch': 0.03}\n{'loss': 0.7334, 'grad_norm': 177619.75, 'learning_rate': 1.3559322033898307e-06, 'epoch': 0.04}\n{'loss': 0.6593, 'grad_norm': 128334.6953125, 'learning_rate': 1.6949152542372882e-06, 'epoch': 0.05}\n{'loss': 0.6908, 'grad_norm': 50682.15234375, 'learning_rate': 2.033898305084746e-06, 'epoch': 0.06}\n{'loss': 0.7049, 'grad_norm': 93968.0, 'learning_rate': 2.372881355932204e-06, 'epoch': 0.07}\n{'loss': 0.6929, 'grad_norm': 80333.4296875, 'learning_rate': 2.7118644067796613e-06, 'epoch': 0.08}\n{'loss': 0.6814, 'grad_norm': 82400.15625, 'learning_rate': 3.0508474576271192e-06, 'epoch': 0.08}\n{'loss': 0.6719, 'grad_norm': 65217.39453125, 'learning_rate': 3.3898305084745763e-06, 'epoch': 0.09}\n{'loss': 0.6712, 'grad_norm': 91660.125, 'learning_rate': 3.7288135593220342e-06, 'epoch': 0.1}\n{'loss': 0.6794, 'grad_norm': 80794.65625, 'learning_rate': 4.067796610169492e-06, 'epoch': 0.11}\n{'loss': 0.7117, 'grad_norm': 79138.578125, 'learning_rate': 4.40677966101695e-06, 'epoch': 0.12}\n{'loss': 0.7257, 'grad_norm': 143087.46875, 'learning_rate': 4.745762711864408e-06, 'epoch': 0.13}\n{'loss': 0.6664, 'grad_norm': 167797.9375, 'learning_rate': 5.084745762711865e-06, 'epoch': 0.14}\n{'loss': 0.7028, 'grad_norm': 177790.34375, 'learning_rate': 5.423728813559323e-06, 'epoch': 0.14}\n{'loss': 0.7025, 'grad_norm': 55030.38671875, 'learning_rate': 5.7627118644067805e-06, 'epoch': 0.15}\n{'loss': 0.6754, 'grad_norm': 90472.828125, 'learning_rate': 6.1016949152542385e-06, 'epoch': 0.16}\n{'loss': 0.7071, 'grad_norm': 89346.234375, 'learning_rate': 6.440677966101695e-06, 'epoch': 0.17}\n{'loss': 0.7002, 'grad_norm': 45651.4375, 'learning_rate': 6.779661016949153e-06, 'epoch': 0.18}\n{'loss': 0.6645, 'grad_norm': 89397.4453125, 'learning_rate': 7.1186440677966106e-06, 'epoch': 0.19}\n{'loss': 0.7354, 'grad_norm': 191163.046875, 'learning_rate': 7.4576271186440685e-06, 'epoch': 0.19}\n{'loss': 0.6346, 'grad_norm': 263997.75, 'learning_rate': 7.796610169491526e-06, 'epoch': 0.2}\n{'loss': 0.7155, 'grad_norm': 136949.03125, 'learning_rate': 8.135593220338983e-06, 'epoch': 0.21}\n{'loss': 0.694, 'grad_norm': 55449.8046875, 'learning_rate': 8.47457627118644e-06, 'epoch': 0.22}\n{'loss': 0.6805, 'grad_norm': 47392.5234375, 'learning_rate': 8.8135593220339e-06, 'epoch': 0.23}\n{'loss': 0.6627, 'grad_norm': 275910.59375, 'learning_rate': 9.152542372881356e-06, 'epoch': 0.24}\n{'loss': 0.6626, 'grad_norm': 101985.8046875, 'learning_rate': 9.491525423728815e-06, 'epoch': 0.25}\n{'loss': 0.7042, 'grad_norm': 136438.171875, 'learning_rate': 9.830508474576272e-06, 'epoch': 0.25}\n{'loss': 0.6473, 'grad_norm': 187242.859375, 'learning_rate': 1.016949152542373e-05, 'epoch': 0.26}\n{'loss': 0.6614, 'grad_norm': 81189.4921875, 'learning_rate': 1.0508474576271188e-05, 'epoch': 0.27}\n{'loss': 0.6893, 'grad_norm': 81794.2734375, 'learning_rate': 1.0847457627118645e-05, 'epoch': 0.28}\n{'loss': 0.694, 'grad_norm': 92552.5546875, 'learning_rate': 1.1186440677966102e-05, 'epoch': 0.29}\n{'loss': 0.6675, 'grad_norm': 83279.6171875, 'learning_rate': 1.1525423728813561e-05, 'epoch': 0.3}\n{'loss': 0.6709, 'grad_norm': 131536.109375, 'learning_rate': 1.1864406779661018e-05, 'epoch': 0.31}\n{'loss': 0.645, 'grad_norm': 214561.578125, 'learning_rate': 1.2203389830508477e-05, 'epoch': 0.31}\n{'loss': 0.7123, 'grad_norm': 132833.5625, 'learning_rate': 1.2542372881355932e-05, 'epoch': 0.32}\n{'loss': 0.7077, 'grad_norm': 99781.8828125, 'learning_rate': 1.288135593220339e-05, 'epoch': 0.33}\n{'loss': 0.7014, 'grad_norm': 60298.9453125, 'learning_rate': 1.3220338983050848e-05, 'epoch': 0.34}\n{'loss': 0.6729, 'grad_norm': 77149.9453125, 'learning_rate': 1.3559322033898305e-05, 'epoch': 0.35}\n{'loss': 0.6759, 'grad_norm': 31363.9765625, 'learning_rate': 1.3898305084745764e-05, 'epoch': 0.36}\n{'loss': 0.7685, 'grad_norm': 260516.0625, 'learning_rate': 1.4237288135593221e-05, 'epoch': 0.36}\n{'loss': 0.6349, 'grad_norm': 219211.15625, 'learning_rate': 1.4576271186440678e-05, 'epoch': 0.37}\n{'loss': 0.6357, 'grad_norm': 125194.765625, 'learning_rate': 1.4915254237288137e-05, 'epoch': 0.38}\n{'loss': 0.7199, 'grad_norm': 141652.46875, 'learning_rate': 1.5254237288135594e-05, 'epoch': 0.39}\n{'loss': 0.7115, 'grad_norm': 79307.65625, 'learning_rate': 1.5593220338983053e-05, 'epoch': 0.4}\n{'loss': 0.7419, 'grad_norm': 216330.0, 'learning_rate': 1.593220338983051e-05, 'epoch': 0.41}\n{'loss': 0.6712, 'grad_norm': 111304.5625, 'learning_rate': 1.6271186440677967e-05, 'epoch': 0.42}\n{'loss': 0.6699, 'grad_norm': 96995.09375, 'learning_rate': 1.6610169491525424e-05, 'epoch': 0.42}\n{'loss': 0.6827, 'grad_norm': 44164.00390625, 'learning_rate': 1.694915254237288e-05, 'epoch': 0.43}\n{'loss': 0.6659, 'grad_norm': 88703.28125, 'learning_rate': 1.728813559322034e-05, 'epoch': 0.44}\n{'loss': 0.7058, 'grad_norm': 120196.21875, 'learning_rate': 1.76271186440678e-05, 'epoch': 0.45}\n{'loss': 0.733, 'grad_norm': 154022.640625, 'learning_rate': 1.7966101694915256e-05, 'epoch': 0.46}\n{'loss': 0.6757, 'grad_norm': 78302.15625, 'learning_rate': 1.8305084745762713e-05, 'epoch': 0.47}\n{'loss': 0.7003, 'grad_norm': 92321.3828125, 'learning_rate': 1.864406779661017e-05, 'epoch': 0.47}\n{'loss': 0.7659, 'grad_norm': 347270.78125, 'learning_rate': 1.898305084745763e-05, 'epoch': 0.48}\n{'loss': 0.6872, 'grad_norm': 111113.3359375, 'learning_rate': 1.9322033898305087e-05, 'epoch': 0.49}\n{'loss': 0.6747, 'grad_norm': 112503.8046875, 'learning_rate': 1.9661016949152545e-05, 'epoch': 0.5}\n{'loss': 0.698, 'grad_norm': 61446.3515625, 'learning_rate': 2e-05, 'epoch': 0.51}\n{'loss': 0.6809, 'grad_norm': 90026.265625, 'learning_rate': 1.9962335216572507e-05, 'epoch': 0.52}\n{'loss': 0.6875, 'grad_norm': 84626.3515625, 'learning_rate': 1.992467043314501e-05, 'epoch': 0.53}\n{'loss': 0.6805, 'grad_norm': 184501.6875, 'learning_rate': 1.9887005649717518e-05, 'epoch': 0.53}\n{'loss': 0.6754, 'grad_norm': 66940.734375, 'learning_rate': 1.984934086629002e-05, 'epoch': 0.54}\n{'loss': 0.6769, 'grad_norm': 51171.80078125, 'learning_rate': 1.9811676082862526e-05, 'epoch': 0.55}\n{'loss': 0.6788, 'grad_norm': 66140.28125, 'learning_rate': 1.977401129943503e-05, 'epoch': 0.56}\n{'loss': 0.6836, 'grad_norm': 67823.5390625, 'learning_rate': 1.9736346516007534e-05, 'epoch': 0.57}\n{'loss': 0.6981, 'grad_norm': 109267.5, 'learning_rate': 1.969868173258004e-05, 'epoch': 0.58}\n{'loss': 0.702, 'grad_norm': 71793.15625, 'learning_rate': 1.9661016949152545e-05, 'epoch': 0.58}\n{'loss': 0.6843, 'grad_norm': 66395.4296875, 'learning_rate': 1.962335216572505e-05, 'epoch': 0.59}\n{'loss': 0.6909, 'grad_norm': 49927.92578125, 'learning_rate': 1.9585687382297552e-05, 'epoch': 0.6}\n{'loss': 0.6842, 'grad_norm': 70135.6875, 'learning_rate': 1.9548022598870058e-05, 'epoch': 0.61}\n{'loss': 0.6878, 'grad_norm': 37475.82421875, 'learning_rate': 1.9510357815442563e-05, 'epoch': 0.62}\n{'loss': 0.6973, 'grad_norm': 62209.76171875, 'learning_rate': 1.9472693032015065e-05, 'epoch': 0.63}\n{'loss': 0.6437, 'grad_norm': 99312.5, 'learning_rate': 1.9435028248587574e-05, 'epoch': 0.64}\n{'loss': 0.7224, 'grad_norm': 144188.171875, 'learning_rate': 1.9397363465160076e-05, 'epoch': 0.64}\n{'loss': 0.6496, 'grad_norm': 105988.421875, 'learning_rate': 1.9359698681732582e-05, 'epoch': 0.65}\n{'loss': 0.6334, 'grad_norm': 187994.203125, 'learning_rate': 1.9322033898305087e-05, 'epoch': 0.66}\n{'loss': 0.6642, 'grad_norm': 103648.09375, 'learning_rate': 1.928436911487759e-05, 'epoch': 0.67}\n{'loss': 0.6974, 'grad_norm': 149771.4375, 'learning_rate': 1.9246704331450095e-05, 'epoch': 0.68}\n{'loss': 0.6951, 'grad_norm': 96070.3203125, 'learning_rate': 1.92090395480226e-05, 'epoch': 0.69}\n{'loss': 0.6771, 'grad_norm': 81160.328125, 'learning_rate': 1.9171374764595106e-05, 'epoch': 0.69}\n{'loss': 0.6742, 'grad_norm': 60771.8359375, 'learning_rate': 1.913370998116761e-05, 'epoch': 0.7}\n{'loss': 0.6314, 'grad_norm': 114532.5703125, 'learning_rate': 1.9096045197740114e-05, 'epoch': 0.71}\n{'loss': 0.6424, 'grad_norm': 101292.609375, 'learning_rate': 1.905838041431262e-05, 'epoch': 0.72}\n{'loss': 0.6812, 'grad_norm': 206913.96875, 'learning_rate': 1.9020715630885125e-05, 'epoch': 0.73}\n{'loss': 0.7047, 'grad_norm': 110694.4609375, 'learning_rate': 1.898305084745763e-05, 'epoch': 0.74}\n{'loss': 0.6737, 'grad_norm': 171181.484375, 'learning_rate': 1.8945386064030133e-05, 'epoch': 0.75}\n{'loss': 0.7178, 'grad_norm': 207883.0, 'learning_rate': 1.8907721280602638e-05, 'epoch': 0.75}\n{'loss': 0.5916, 'grad_norm': 237610.375, 'learning_rate': 1.8870056497175144e-05, 'epoch': 0.76}\n{'loss': 0.7165, 'grad_norm': 120140.0234375, 'learning_rate': 1.8832391713747646e-05, 'epoch': 0.77}\n{'loss': 0.7129, 'grad_norm': 221183.859375, 'learning_rate': 1.8794726930320155e-05, 'epoch': 0.78}\n{'loss': 0.6671, 'grad_norm': 106986.6015625, 'learning_rate': 1.8757062146892657e-05, 'epoch': 0.79}\n{'loss': 0.7111, 'grad_norm': 178509.78125, 'learning_rate': 1.8719397363465162e-05, 'epoch': 0.8}\n{'loss': 0.6305, 'grad_norm': 130291.703125, 'learning_rate': 1.8681732580037664e-05, 'epoch': 0.81}\n{'loss': 0.6782, 'grad_norm': 95761.8359375, 'learning_rate': 1.864406779661017e-05, 'epoch': 0.81}\n{'loss': 0.6026, 'grad_norm': 256776.703125, 'learning_rate': 1.8606403013182675e-05, 'epoch': 0.82}\n{'loss': 0.5841, 'grad_norm': 284040.40625, 'learning_rate': 1.856873822975518e-05, 'epoch': 0.83}\n{'loss': 0.7853, 'grad_norm': 282438.65625, 'learning_rate': 1.8531073446327686e-05, 'epoch': 0.84}\n{'loss': 0.6477, 'grad_norm': 150449.9375, 'learning_rate': 1.849340866290019e-05, 'epoch': 0.85}\n{'loss': 0.6554, 'grad_norm': 75002.6484375, 'learning_rate': 1.8455743879472694e-05, 'epoch': 0.86}\n{'loss': 0.6153, 'grad_norm': 126470.1015625, 'learning_rate': 1.84180790960452e-05, 'epoch': 0.86}\n{'loss': 0.795, 'grad_norm': 287804.125, 'learning_rate': 1.8380414312617705e-05, 'epoch': 0.87}\n{'loss': 0.6671, 'grad_norm': 135381.328125, 'learning_rate': 1.834274952919021e-05, 'epoch': 0.88}\n{'loss': 0.6433, 'grad_norm': 86548.9921875, 'learning_rate': 1.8305084745762713e-05, 'epoch': 0.89}\n{'loss': 0.6948, 'grad_norm': 262348.59375, 'learning_rate': 1.826741996233522e-05, 'epoch': 0.9}\n{'loss': 0.7335, 'grad_norm': 189537.265625, 'learning_rate': 1.822975517890772e-05, 'epoch': 0.91}\n{'loss': 0.6207, 'grad_norm': 266373.71875, 'learning_rate': 1.8192090395480226e-05, 'epoch': 0.92}\n{'loss': 0.6491, 'grad_norm': 116293.109375, 'learning_rate': 1.815442561205273e-05, 'epoch': 0.92}\n{'loss': 0.7351, 'grad_norm': 221352.71875, 'learning_rate': 1.8116760828625237e-05, 'epoch': 0.93}\n{'loss': 0.695, 'grad_norm': 111794.1953125, 'learning_rate': 1.8079096045197743e-05, 'epoch': 0.94}\n{'loss': 0.645, 'grad_norm': 96578.171875, 'learning_rate': 1.8041431261770245e-05, 'epoch': 0.95}\n{'loss': 0.6702, 'grad_norm': 76653.3984375, 'learning_rate': 1.800376647834275e-05, 'epoch': 0.96}\n{'loss': 0.7212, 'grad_norm': 190919.671875, 'learning_rate': 1.7966101694915256e-05, 'epoch': 0.97}\n{'loss': 0.7267, 'grad_norm': 133090.921875, 'learning_rate': 1.792843691148776e-05, 'epoch': 0.97}\n{'loss': 0.7206, 'grad_norm': 157682.09375, 'learning_rate': 1.7890772128060267e-05, 'epoch': 0.98}\n{'loss': 0.5803, 'grad_norm': 175017.984375, 'learning_rate': 1.785310734463277e-05, 'epoch': 0.99}\n{'loss': 0.6815, 'grad_norm': 401203.46875, 'learning_rate': 1.7815442561205274e-05, 'epoch': 1.0}\n{'loss': 0.6414, 'grad_norm': 155500.78125, 'learning_rate': 1.7777777777777777e-05, 'epoch': 1.01}\n{'loss': 0.6546, 'grad_norm': 94512.4609375, 'learning_rate': 1.7740112994350286e-05, 'epoch': 1.02}\n{'loss': 0.6514, 'grad_norm': 78687.0078125, 'learning_rate': 1.7702448210922788e-05, 'epoch': 1.03}\n{'loss': 0.6884, 'grad_norm': 155261.921875, 'learning_rate': 1.7664783427495293e-05, 'epoch': 1.03}\n{'loss': 0.6733, 'grad_norm': 66101.6953125, 'learning_rate': 1.76271186440678e-05, 'epoch': 1.04}\n{'loss': 0.6743, 'grad_norm': 77029.4453125, 'learning_rate': 1.75894538606403e-05, 'epoch': 1.05}\n{'loss': 0.6376, 'grad_norm': 93881.5703125, 'learning_rate': 1.755178907721281e-05, 'epoch': 1.06}\n{'loss': 0.6466, 'grad_norm': 120269.796875, 'learning_rate': 1.7514124293785312e-05, 'epoch': 1.07}\n{'loss': 0.6339, 'grad_norm': 181037.859375, 'learning_rate': 1.7476459510357817e-05, 'epoch': 1.08}\n{'loss': 0.6178, 'grad_norm': 98860.3203125, 'learning_rate': 1.7438794726930323e-05, 'epoch': 1.08}\n{'loss': 0.707, 'grad_norm': 81445.4296875, 'learning_rate': 1.7401129943502825e-05, 'epoch': 1.09}\n{'loss': 0.6243, 'grad_norm': 88909.4140625, 'learning_rate': 1.736346516007533e-05, 'epoch': 1.1}\n{'loss': 0.6036, 'grad_norm': 131112.828125, 'learning_rate': 1.7325800376647836e-05, 'epoch': 1.11}\n{'loss': 0.5871, 'grad_norm': 182182.484375, 'learning_rate': 1.728813559322034e-05, 'epoch': 1.12}\n{'loss': 0.5378, 'grad_norm': 188527.265625, 'learning_rate': 1.7250470809792844e-05, 'epoch': 1.13}\n{'loss': 0.597, 'grad_norm': 122857.765625, 'learning_rate': 1.721280602636535e-05, 'epoch': 1.14}\n{'loss': 0.5901, 'grad_norm': 150497.625, 'learning_rate': 1.7175141242937855e-05, 'epoch': 1.14}\n{'loss': 0.532, 'grad_norm': 119944.6796875, 'learning_rate': 1.7137476459510357e-05, 'epoch': 1.15}\n{'loss': 0.5642, 'grad_norm': 204028.0, 'learning_rate': 1.7099811676082866e-05, 'epoch': 1.16}\n{'loss': 0.5985, 'grad_norm': 118133.859375, 'learning_rate': 1.7062146892655368e-05, 'epoch': 1.17}\n{'loss': 0.5843, 'grad_norm': 174857.765625, 'learning_rate': 1.7024482109227873e-05, 'epoch': 1.18}\n{'loss': 0.5539, 'grad_norm': 159495.375, 'learning_rate': 1.698681732580038e-05, 'epoch': 1.19}\n{'loss': 0.5048, 'grad_norm': 136245.21875, 'learning_rate': 1.694915254237288e-05, 'epoch': 1.19}\n{'loss': 0.7722, 'grad_norm': 197882.109375, 'learning_rate': 1.691148775894539e-05, 'epoch': 1.2}\n{'loss': 0.7171, 'grad_norm': 197140.09375, 'learning_rate': 1.6873822975517892e-05, 'epoch': 1.21}\n{'loss': 0.6441, 'grad_norm': 317521.59375, 'learning_rate': 1.6836158192090398e-05, 'epoch': 1.22}\n{'loss': 0.8013, 'grad_norm': 318531.375, 'learning_rate': 1.67984934086629e-05, 'epoch': 1.23}\n{'loss': 0.5497, 'grad_norm': 231132.234375, 'learning_rate': 1.6760828625235405e-05, 'epoch': 1.24}\n{'loss': 0.6037, 'grad_norm': 296207.84375, 'learning_rate': 1.672316384180791e-05, 'epoch': 1.25}\n{'loss': 0.5527, 'grad_norm': 246194.71875, 'learning_rate': 1.6685499058380416e-05, 'epoch': 1.25}\n{'loss': 0.542, 'grad_norm': 232926.828125, 'learning_rate': 1.6647834274952922e-05, 'epoch': 1.26}\n{'loss': 0.6303, 'grad_norm': 283656.53125, 'learning_rate': 1.6610169491525424e-05, 'epoch': 1.27}\n{'loss': 0.5426, 'grad_norm': 383624.34375, 'learning_rate': 1.657250470809793e-05, 'epoch': 1.28}\n{'loss': 0.6171, 'grad_norm': 302384.9375, 'learning_rate': 1.6534839924670435e-05, 'epoch': 1.29}\n{'loss': 0.6562, 'grad_norm': 310726.78125, 'learning_rate': 1.6497175141242937e-05, 'epoch': 1.3}\n{'loss': 0.5995, 'grad_norm': 196180.125, 'learning_rate': 1.6459510357815446e-05, 'epoch': 1.31}\n{'loss': 0.4446, 'grad_norm': 256101.421875, 'learning_rate': 1.6421845574387948e-05, 'epoch': 1.31}\n{'loss': 0.5378, 'grad_norm': 282434.09375, 'learning_rate': 1.6384180790960454e-05, 'epoch': 1.32}\n{'loss': 0.5666, 'grad_norm': 259719.765625, 'learning_rate': 1.634651600753296e-05, 'epoch': 1.33}\n{'loss': 0.4016, 'grad_norm': 161229.859375, 'learning_rate': 1.630885122410546e-05, 'epoch': 1.34}\n{'loss': 0.3765, 'grad_norm': 146618.0, 'learning_rate': 1.6271186440677967e-05, 'epoch': 1.35}\n{'loss': 0.3547, 'grad_norm': 184216.65625, 'learning_rate': 1.6233521657250472e-05, 'epoch': 1.36}\n{'loss': 0.8461, 'grad_norm': 295715.9375, 'learning_rate': 1.6195856873822978e-05, 'epoch': 1.36}\n{'loss': 0.5227, 'grad_norm': 416612.53125, 'learning_rate': 1.615819209039548e-05, 'epoch': 1.37}\n{'loss': 0.7406, 'grad_norm': 273360.03125, 'learning_rate': 1.6120527306967986e-05, 'epoch': 1.38}\n{'loss': 0.7596, 'grad_norm': 296749.875, 'learning_rate': 1.608286252354049e-05, 'epoch': 1.39}\n{'loss': 0.5858, 'grad_norm': 301580.53125, 'learning_rate': 1.6045197740112997e-05, 'epoch': 1.4}\n{'loss': 0.5903, 'grad_norm': 223033.875, 'learning_rate': 1.6007532956685502e-05, 'epoch': 1.41}\n{'loss': 0.7963, 'grad_norm': 586587.75, 'learning_rate': 1.5969868173258004e-05, 'epoch': 1.42}\n{'loss': 0.6517, 'grad_norm': 378420.0625, 'learning_rate': 1.593220338983051e-05, 'epoch': 1.42}\n{'loss': 0.6414, 'grad_norm': 261141.453125, 'learning_rate': 1.5894538606403015e-05, 'epoch': 1.43}\n{'loss': 0.5803, 'grad_norm': 338596.8125, 'learning_rate': 1.5856873822975518e-05, 'epoch': 1.44}\n{'loss': 0.439, 'grad_norm': 218028.4375, 'learning_rate': 1.5819209039548023e-05, 'epoch': 1.45}\n{'loss': 0.4717, 'grad_norm': 176752.265625, 'learning_rate': 1.578154425612053e-05, 'epoch': 1.46}\n{'loss': 0.5352, 'grad_norm': 203121.6875, 'learning_rate': 1.5743879472693034e-05, 'epoch': 1.47}\n{'loss': 0.6055, 'grad_norm': 372177.375, 'learning_rate': 1.5706214689265536e-05, 'epoch': 1.47}\n{'loss': 0.5857, 'grad_norm': 306576.34375, 'learning_rate': 1.5668549905838042e-05, 'epoch': 1.48}\n{'loss': 0.6297, 'grad_norm': 208675.453125, 'learning_rate': 1.5630885122410547e-05, 'epoch': 1.49}\n{'loss': 0.5267, 'grad_norm': 138209.859375, 'learning_rate': 1.5593220338983053e-05, 'epoch': 1.5}\n{'loss': 0.5163, 'grad_norm': 201607.203125, 'learning_rate': 1.555555555555556e-05, 'epoch': 1.51}\n{'loss': 0.6421, 'grad_norm': 277884.96875, 'learning_rate': 1.551789077212806e-05, 'epoch': 1.52}\n{'loss': 0.6234, 'grad_norm': 288741.75, 'learning_rate': 1.5480225988700566e-05, 'epoch': 1.53}\n{'loss': 0.4632, 'grad_norm': 192802.921875, 'learning_rate': 1.544256120527307e-05, 'epoch': 1.53}\n{'loss': 0.6164, 'grad_norm': 194981.71875, 'learning_rate': 1.5404896421845577e-05, 'epoch': 1.54}\n{'loss': 0.4261, 'grad_norm': 206713.671875, 'learning_rate': 1.536723163841808e-05, 'epoch': 1.55}\n{'loss': 0.6927, 'grad_norm': 272184.96875, 'learning_rate': 1.5329566854990585e-05, 'epoch': 1.56}\n{'loss': 0.4506, 'grad_norm': 362270.125, 'learning_rate': 1.529190207156309e-05, 'epoch': 1.57}\n{'loss': 0.4899, 'grad_norm': 177090.75, 'learning_rate': 1.5254237288135594e-05, 'epoch': 1.58}\n{'loss': 0.5061, 'grad_norm': 236463.671875, 'learning_rate': 1.5216572504708098e-05, 'epoch': 1.58}\n{'loss': 0.4448, 'grad_norm': 236785.375, 'learning_rate': 1.5178907721280605e-05, 'epoch': 1.59}\n{'loss': 0.3425, 'grad_norm': 293473.75, 'learning_rate': 1.5141242937853109e-05, 'epoch': 1.6}\n{'loss': 0.5521, 'grad_norm': 283669.21875, 'learning_rate': 1.5103578154425613e-05, 'epoch': 1.61}\n{'loss': 0.5652, 'grad_norm': 491841.46875, 'learning_rate': 1.5065913370998118e-05, 'epoch': 1.62}\n{'loss': 0.6223, 'grad_norm': 474405.71875, 'learning_rate': 1.5028248587570622e-05, 'epoch': 1.63}\n{'loss': 0.4217, 'grad_norm': 219422.921875, 'learning_rate': 1.4990583804143128e-05, 'epoch': 1.64}\n{'loss': 0.3312, 'grad_norm': 276476.8125, 'learning_rate': 1.4952919020715633e-05, 'epoch': 1.64}\n{'loss': 0.4179, 'grad_norm': 459672.9375, 'learning_rate': 1.4915254237288137e-05, 'epoch': 1.65}\n{'loss': 0.4564, 'grad_norm': 481746.15625, 'learning_rate': 1.487758945386064e-05, 'epoch': 1.66}\n{'loss': 0.8538, 'grad_norm': 549900.8125, 'learning_rate': 1.4839924670433146e-05, 'epoch': 1.67}\n{'loss': 0.4191, 'grad_norm': 383889.5625, 'learning_rate': 1.480225988700565e-05, 'epoch': 1.68}\n{'loss': 0.6261, 'grad_norm': 795993.0, 'learning_rate': 1.4764595103578157e-05, 'epoch': 1.69}\n{'loss': 0.3393, 'grad_norm': 163396.0, 'learning_rate': 1.4726930320150661e-05, 'epoch': 1.69}\n{'loss': 0.4952, 'grad_norm': 675574.8125, 'learning_rate': 1.4689265536723165e-05, 'epoch': 1.7}\n{'loss': 0.5795, 'grad_norm': 615277.25, 'learning_rate': 1.4651600753295669e-05, 'epoch': 1.71}\n{'loss': 0.4424, 'grad_norm': 512042.03125, 'learning_rate': 1.4613935969868174e-05, 'epoch': 1.72}\n{'loss': 0.4081, 'grad_norm': 325230.59375, 'learning_rate': 1.4576271186440678e-05, 'epoch': 1.73}\n{'loss': 0.3876, 'grad_norm': 412127.3125, 'learning_rate': 1.4538606403013185e-05, 'epoch': 1.74}\n{'loss': 0.4018, 'grad_norm': 354059.90625, 'learning_rate': 1.450094161958569e-05, 'epoch': 1.75}\n{'loss': 0.3458, 'grad_norm': 327898.8125, 'learning_rate': 1.4463276836158193e-05, 'epoch': 1.75}\n{'loss': 0.447, 'grad_norm': 361680.5625, 'learning_rate': 1.4425612052730697e-05, 'epoch': 1.76}\n{'loss': 0.2928, 'grad_norm': 245622.3125, 'learning_rate': 1.4387947269303202e-05, 'epoch': 1.77}\n{'loss': 0.6066, 'grad_norm': 359274.125, 'learning_rate': 1.4350282485875708e-05, 'epoch': 1.78}\n{'loss': 0.5805, 'grad_norm': 442013.96875, 'learning_rate': 1.4312617702448213e-05, 'epoch': 1.79}\n{'loss': 0.474, 'grad_norm': 266817.625, 'learning_rate': 1.4274952919020717e-05, 'epoch': 1.8}\n{'loss': 0.5542, 'grad_norm': 580317.625, 'learning_rate': 1.4237288135593221e-05, 'epoch': 1.81}\n{'loss': 0.4598, 'grad_norm': 279387.0, 'learning_rate': 1.4199623352165725e-05, 'epoch': 1.81}\n{'loss': 0.314, 'grad_norm': 276277.40625, 'learning_rate': 1.416195856873823e-05, 'epoch': 1.82}\n{'loss': 0.5637, 'grad_norm': 741583.1875, 'learning_rate': 1.4124293785310736e-05, 'epoch': 1.83}\n{'loss': 0.5536, 'grad_norm': 291433.96875, 'learning_rate': 1.4086629001883241e-05, 'epoch': 1.84}\n{'loss': 0.3394, 'grad_norm': 249827.828125, 'learning_rate': 1.4048964218455745e-05, 'epoch': 1.85}\n{'loss': 0.6013, 'grad_norm': 283700.59375, 'learning_rate': 1.4011299435028249e-05, 'epoch': 1.86}\n{'loss': 0.6842, 'grad_norm': 412990.5625, 'learning_rate': 1.3973634651600753e-05, 'epoch': 1.86}\n{'loss': 0.6974, 'grad_norm': 302649.03125, 'learning_rate': 1.393596986817326e-05, 'epoch': 1.87}\n{'loss': 0.3355, 'grad_norm': 338202.53125, 'learning_rate': 1.3898305084745764e-05, 'epoch': 1.88}\n{'loss': 0.5687, 'grad_norm': 374209.5625, 'learning_rate': 1.386064030131827e-05, 'epoch': 1.89}\n{'loss': 0.5448, 'grad_norm': 248577.828125, 'learning_rate': 1.3822975517890773e-05, 'epoch': 1.9}\n{'loss': 0.5384, 'grad_norm': 436414.90625, 'learning_rate': 1.3785310734463277e-05, 'epoch': 1.91}\n{'loss': 0.4386, 'grad_norm': 411005.28125, 'learning_rate': 1.3747645951035781e-05, 'epoch': 1.92}\n{'loss': 0.4651, 'grad_norm': 211238.640625, 'learning_rate': 1.3709981167608288e-05, 'epoch': 1.92}\n{'loss': 0.3628, 'grad_norm': 226414.234375, 'learning_rate': 1.3672316384180792e-05, 'epoch': 1.93}\n{'loss': 0.6858, 'grad_norm': 275847.625, 'learning_rate': 1.3634651600753298e-05, 'epoch': 1.94}\n{'loss': 0.3402, 'grad_norm': 368198.46875, 'learning_rate': 1.3596986817325801e-05, 'epoch': 1.95}\n{'loss': 0.766, 'grad_norm': 642095.0625, 'learning_rate': 1.3559322033898305e-05, 'epoch': 1.96}\n{'loss': 0.4124, 'grad_norm': 244202.171875, 'learning_rate': 1.3521657250470809e-05, 'epoch': 1.97}\n{'loss': 0.3422, 'grad_norm': 220392.953125, 'learning_rate': 1.3483992467043316e-05, 'epoch': 1.97}\n{'loss': 0.4672, 'grad_norm': 352915.4375, 'learning_rate': 1.344632768361582e-05, 'epoch': 1.98}\n{'loss': 0.6235, 'grad_norm': 544997.375, 'learning_rate': 1.3408662900188326e-05, 'epoch': 1.99}\n{'loss': 0.5381, 'grad_norm': 904075.75, 'learning_rate': 1.337099811676083e-05, 'epoch': 2.0}\n{'loss': 0.2615, 'grad_norm': 211018.296875, 'learning_rate': 1.3333333333333333e-05, 'epoch': 2.01}\n{'loss': 0.5665, 'grad_norm': 295509.15625, 'learning_rate': 1.329566854990584e-05, 'epoch': 2.02}\n{'loss': 0.4924, 'grad_norm': 347794.9375, 'learning_rate': 1.3258003766478344e-05, 'epoch': 2.03}\n{'loss': 0.5216, 'grad_norm': 530157.0, 'learning_rate': 1.3220338983050848e-05, 'epoch': 2.03}\n{'loss': 0.594, 'grad_norm': 821995.875, 'learning_rate': 1.3182674199623354e-05, 'epoch': 2.04}\n{'loss': 0.6412, 'grad_norm': 482504.96875, 'learning_rate': 1.3145009416195858e-05, 'epoch': 2.05}\n{'loss': 0.392, 'grad_norm': 590305.3125, 'learning_rate': 1.3107344632768361e-05, 'epoch': 2.06}\n{'loss': 0.3097, 'grad_norm': 215365.265625, 'learning_rate': 1.3069679849340869e-05, 'epoch': 2.07}\n{'loss': 0.375, 'grad_norm': 465933.34375, 'learning_rate': 1.3032015065913372e-05, 'epoch': 2.08}\n{'loss': 0.3648, 'grad_norm': 246755.328125, 'learning_rate': 1.2994350282485876e-05, 'epoch': 2.08}\n{'loss': 0.2694, 'grad_norm': 297482.53125, 'learning_rate': 1.2956685499058382e-05, 'epoch': 2.09}\n{'loss': 0.4379, 'grad_norm': 370021.4375, 'learning_rate': 1.2919020715630886e-05, 'epoch': 2.1}\n{'loss': 0.2691, 'grad_norm': 329867.5, 'learning_rate': 1.288135593220339e-05, 'epoch': 2.11}\n{'loss': 0.3254, 'grad_norm': 347515.8125, 'learning_rate': 1.2843691148775897e-05, 'epoch': 2.12}\n{'loss': 0.2673, 'grad_norm': 473664.9375, 'learning_rate': 1.28060263653484e-05, 'epoch': 2.13}\n{'loss': 0.2233, 'grad_norm': 267101.875, 'learning_rate': 1.2768361581920904e-05, 'epoch': 2.14}\n{'loss': 0.3447, 'grad_norm': 461417.75, 'learning_rate': 1.273069679849341e-05, 'epoch': 2.14}\n{'loss': 0.2957, 'grad_norm': 307558.78125, 'learning_rate': 1.2693032015065914e-05, 'epoch': 2.15}\n{'loss': 0.6535, 'grad_norm': 544043.75, 'learning_rate': 1.265536723163842e-05, 'epoch': 2.16}\n{'loss': 0.4781, 'grad_norm': 515194.96875, 'learning_rate': 1.2617702448210925e-05, 'epoch': 2.17}\n{'loss': 0.3567, 'grad_norm': 418489.875, 'learning_rate': 1.2580037664783428e-05, 'epoch': 2.18}\n{'loss': 0.2426, 'grad_norm': 171642.25, 'learning_rate': 1.2542372881355932e-05, 'epoch': 2.19}\n{'loss': 0.5719, 'grad_norm': 350339.1875, 'learning_rate': 1.2504708097928438e-05, 'epoch': 2.19}\n{'loss': 0.7692, 'grad_norm': 976454.4375, 'learning_rate': 1.2467043314500942e-05, 'epoch': 2.2}\n{'loss': 0.2416, 'grad_norm': 606391.875, 'learning_rate': 1.2429378531073449e-05, 'epoch': 2.21}\n{'loss': 0.3401, 'grad_norm': 239356.1875, 'learning_rate': 1.2391713747645953e-05, 'epoch': 2.22}\n{'loss': 0.311, 'grad_norm': 361819.09375, 'learning_rate': 1.2354048964218457e-05, 'epoch': 2.23}\n{'loss': 0.3908, 'grad_norm': 426884.3125, 'learning_rate': 1.2316384180790962e-05, 'epoch': 2.24}\n{'loss': 0.56, 'grad_norm': 414412.34375, 'learning_rate': 1.2278719397363466e-05, 'epoch': 2.25}\n{'loss': 0.5961, 'grad_norm': 493843.0625, 'learning_rate': 1.224105461393597e-05, 'epoch': 2.25}\n{'loss': 0.2943, 'grad_norm': 539959.125, 'learning_rate': 1.2203389830508477e-05, 'epoch': 2.26}\n{'loss': 0.3574, 'grad_norm': 457554.46875, 'learning_rate': 1.216572504708098e-05, 'epoch': 2.27}\n{'loss': 0.3364, 'grad_norm': 641189.3125, 'learning_rate': 1.2128060263653485e-05, 'epoch': 2.28}\n{'loss': 0.4305, 'grad_norm': 278176.1875, 'learning_rate': 1.209039548022599e-05, 'epoch': 2.29}\n{'loss': 0.3236, 'grad_norm': 331459.34375, 'learning_rate': 1.2052730696798494e-05, 'epoch': 2.3}\n{'loss': 0.6151, 'grad_norm': 356766.4375, 'learning_rate': 1.2015065913371e-05, 'epoch': 2.31}\n{'loss': 0.2279, 'grad_norm': 193890.4375, 'learning_rate': 1.1977401129943505e-05, 'epoch': 2.31}\n{'loss': 0.6071, 'grad_norm': 1154574.5, 'learning_rate': 1.1939736346516009e-05, 'epoch': 2.32}\n{'loss': 0.4524, 'grad_norm': 370880.6875, 'learning_rate': 1.1902071563088513e-05, 'epoch': 2.33}\n{'loss': 0.3704, 'grad_norm': 432030.75, 'learning_rate': 1.1864406779661018e-05, 'epoch': 2.34}\n{'loss': 0.2323, 'grad_norm': 225215.84375, 'learning_rate': 1.1826741996233522e-05, 'epoch': 2.35}\n{'loss': 0.4768, 'grad_norm': 386439.8125, 'learning_rate': 1.1789077212806027e-05, 'epoch': 2.36}\n{'loss': 0.4563, 'grad_norm': 640711.4375, 'learning_rate': 1.1751412429378533e-05, 'epoch': 2.36}\n{'loss': 0.3804, 'grad_norm': 278093.34375, 'learning_rate': 1.1713747645951037e-05, 'epoch': 2.37}\n{'loss': 0.4742, 'grad_norm': 423787.65625, 'learning_rate': 1.167608286252354e-05, 'epoch': 2.38}\n{'loss': 0.444, 'grad_norm': 758493.25, 'learning_rate': 1.1638418079096046e-05, 'epoch': 2.39}\n{'loss': 0.5888, 'grad_norm': 462793.5625, 'learning_rate': 1.160075329566855e-05, 'epoch': 2.4}\n{'loss': 0.4758, 'grad_norm': 577414.25, 'learning_rate': 1.1563088512241056e-05, 'epoch': 2.41}\n{'loss': 0.3168, 'grad_norm': 241200.859375, 'learning_rate': 1.1525423728813561e-05, 'epoch': 2.42}\n{'loss': 0.276, 'grad_norm': 274919.84375, 'learning_rate': 1.1487758945386065e-05, 'epoch': 2.42}\n{'loss': 0.2918, 'grad_norm': 192525.921875, 'learning_rate': 1.1450094161958569e-05, 'epoch': 2.43}\n{'loss': 0.3769, 'grad_norm': 318258.78125, 'learning_rate': 1.1412429378531074e-05, 'epoch': 2.44}\n{'loss': 0.407, 'grad_norm': 213262.078125, 'learning_rate': 1.137476459510358e-05, 'epoch': 2.45}\n{'loss': 0.2853, 'grad_norm': 417548.34375, 'learning_rate': 1.1337099811676084e-05, 'epoch': 2.46}\n{'loss': 0.3329, 'grad_norm': 351330.34375, 'learning_rate': 1.1299435028248589e-05, 'epoch': 2.47}\n{'loss': 0.2127, 'grad_norm': 289296.4375, 'learning_rate': 1.1261770244821093e-05, 'epoch': 2.47}\n{'loss': 0.5619, 'grad_norm': 805018.75, 'learning_rate': 1.1224105461393597e-05, 'epoch': 2.48}\n{'loss': 0.2921, 'grad_norm': 233373.046875, 'learning_rate': 1.1186440677966102e-05, 'epoch': 2.49}\n{'loss': 0.3986, 'grad_norm': 412467.09375, 'learning_rate': 1.1148775894538608e-05, 'epoch': 2.5}\n{'loss': 0.3398, 'grad_norm': 625359.0, 'learning_rate': 1.1111111111111113e-05, 'epoch': 2.51}\n{'loss': 0.2859, 'grad_norm': 389432.8125, 'learning_rate': 1.1073446327683617e-05, 'epoch': 2.52}\n{'loss': 0.4201, 'grad_norm': 456063.4375, 'learning_rate': 1.1035781544256121e-05, 'epoch': 2.53}\n{'loss': 0.6332, 'grad_norm': 612903.0, 'learning_rate': 1.0998116760828625e-05, 'epoch': 2.53}\n{'loss': 0.3368, 'grad_norm': 384071.40625, 'learning_rate': 1.096045197740113e-05, 'epoch': 2.54}\n{'loss': 0.4945, 'grad_norm': 441738.9375, 'learning_rate': 1.0922787193973636e-05, 'epoch': 2.55}\n{'loss': 0.482, 'grad_norm': 274289.375, 'learning_rate': 1.0885122410546141e-05, 'epoch': 2.56}\n{'loss': 0.3212, 'grad_norm': 263074.3125, 'learning_rate': 1.0847457627118645e-05, 'epoch': 2.57}\n{'loss': 0.4579, 'grad_norm': 773343.1875, 'learning_rate': 1.0809792843691149e-05, 'epoch': 2.58}\n{'loss': 0.692, 'grad_norm': 488306.09375, 'learning_rate': 1.0772128060263653e-05, 'epoch': 2.58}\n{'loss': 0.3907, 'grad_norm': 464096.9375, 'learning_rate': 1.073446327683616e-05, 'epoch': 2.59}\n{'loss': 0.3517, 'grad_norm': 293326.75, 'learning_rate': 1.0696798493408664e-05, 'epoch': 2.6}\n{'loss': 0.4731, 'grad_norm': 295207.78125, 'learning_rate': 1.065913370998117e-05, 'epoch': 2.61}\n{'loss': 0.3882, 'grad_norm': 414152.09375, 'learning_rate': 1.0621468926553673e-05, 'epoch': 2.62}\n{'loss': 0.4082, 'grad_norm': 379755.21875, 'learning_rate': 1.0583804143126177e-05, 'epoch': 2.63}\n{'loss': 0.256, 'grad_norm': 331964.03125, 'learning_rate': 1.0546139359698681e-05, 'epoch': 2.64}\n{'loss': 0.3273, 'grad_norm': 346724.03125, 'learning_rate': 1.0508474576271188e-05, 'epoch': 2.64}\n{'loss': 0.5391, 'grad_norm': 304058.96875, 'learning_rate': 1.0470809792843692e-05, 'epoch': 2.65}\n{'loss': 0.6028, 'grad_norm': 244149.734375, 'learning_rate': 1.0433145009416197e-05, 'epoch': 2.66}\n{'loss': 0.3351, 'grad_norm': 374382.5625, 'learning_rate': 1.0395480225988701e-05, 'epoch': 2.67}\n{'loss': 0.7137, 'grad_norm': 450675.9375, 'learning_rate': 1.0357815442561205e-05, 'epoch': 2.68}\n{'loss': 0.2416, 'grad_norm': 215224.078125, 'learning_rate': 1.0320150659133709e-05, 'epoch': 2.69}\n{'loss': 0.1731, 'grad_norm': 231496.46875, 'learning_rate': 1.0282485875706216e-05, 'epoch': 2.69}\n{'loss': 0.398, 'grad_norm': 424156.6875, 'learning_rate': 1.024482109227872e-05, 'epoch': 2.7}\n{'loss': 0.4406, 'grad_norm': 415768.5, 'learning_rate': 1.0207156308851226e-05, 'epoch': 2.71}\n{'loss': 0.5506, 'grad_norm': 489913.25, 'learning_rate': 1.016949152542373e-05, 'epoch': 2.72}\n{'loss': 0.8082, 'grad_norm': 686585.375, 'learning_rate': 1.0131826741996233e-05, 'epoch': 2.73}\n{'loss': 0.2405, 'grad_norm': 381044.59375, 'learning_rate': 1.009416195856874e-05, 'epoch': 2.74}\n{'loss': 0.1497, 'grad_norm': 284502.28125, 'learning_rate': 1.0056497175141244e-05, 'epoch': 2.75}\n{'loss': 0.4208, 'grad_norm': 204274.859375, 'learning_rate': 1.0018832391713748e-05, 'epoch': 2.75}\n{'loss': 0.4553, 'grad_norm': 347029.25, 'learning_rate': 9.981167608286254e-06, 'epoch': 2.76}\n{'loss': 0.3543, 'grad_norm': 322025.96875, 'learning_rate': 9.943502824858759e-06, 'epoch': 2.77}\n{'loss': 0.5328, 'grad_norm': 359591.9375, 'learning_rate': 9.905838041431263e-06, 'epoch': 2.78}\n{'loss': 0.277, 'grad_norm': 255473.546875, 'learning_rate': 9.868173258003767e-06, 'epoch': 2.79}\n{'loss': 0.4482, 'grad_norm': 502249.1875, 'learning_rate': 9.830508474576272e-06, 'epoch': 2.8}\n{'loss': 0.3629, 'grad_norm': 581638.75, 'learning_rate': 9.792843691148776e-06, 'epoch': 2.81}\n{'loss': 0.4101, 'grad_norm': 419497.9375, 'learning_rate': 9.755178907721282e-06, 'epoch': 2.81}\n{'loss': 0.2306, 'grad_norm': 196297.9375, 'learning_rate': 9.717514124293787e-06, 'epoch': 2.82}\n{'loss': 0.1694, 'grad_norm': 182169.96875, 'learning_rate': 9.679849340866291e-06, 'epoch': 2.83}\n{'loss': 0.343, 'grad_norm': 349100.5625, 'learning_rate': 9.642184557438795e-06, 'epoch': 2.84}\n{'loss': 0.2726, 'grad_norm': 266414.71875, 'learning_rate': 9.6045197740113e-06, 'epoch': 2.85}\n{'loss': 0.6395, 'grad_norm': 314293.875, 'learning_rate': 9.566854990583804e-06, 'epoch': 2.86}\n{'loss': 0.3084, 'grad_norm': 376781.3125, 'learning_rate': 9.52919020715631e-06, 'epoch': 2.86}\n{'loss': 0.4149, 'grad_norm': 432869.5, 'learning_rate': 9.491525423728815e-06, 'epoch': 2.87}\n{'loss': 0.2465, 'grad_norm': 309667.28125, 'learning_rate': 9.453860640301319e-06, 'epoch': 2.88}\n{'loss': 0.1631, 'grad_norm': 235943.765625, 'learning_rate': 9.416195856873823e-06, 'epoch': 2.89}\n{'loss': 0.3867, 'grad_norm': 356177.78125, 'learning_rate': 9.378531073446328e-06, 'epoch': 2.9}\n{'loss': 0.2677, 'grad_norm': 257762.78125, 'learning_rate': 9.340866290018832e-06, 'epoch': 2.91}\n{'loss': 0.293, 'grad_norm': 186245.921875, 'learning_rate': 9.303201506591338e-06, 'epoch': 2.92}\n{'loss': 0.3776, 'grad_norm': 455777.6875, 'learning_rate': 9.265536723163843e-06, 'epoch': 2.92}\n{'loss': 0.4614, 'grad_norm': 425649.9375, 'learning_rate': 9.227871939736347e-06, 'epoch': 2.93}\n{'loss': 0.4431, 'grad_norm': 303557.9375, 'learning_rate': 9.190207156308853e-06, 'epoch': 2.94}\n{'loss': 0.2741, 'grad_norm': 291098.375, 'learning_rate': 9.152542372881356e-06, 'epoch': 2.95}\n{'loss': 0.2328, 'grad_norm': 171854.5625, 'learning_rate': 9.11487758945386e-06, 'epoch': 2.96}\n{'loss': 0.3205, 'grad_norm': 659094.375, 'learning_rate': 9.077212806026366e-06, 'epoch': 2.97}\n{'loss': 0.4801, 'grad_norm': 350019.46875, 'learning_rate': 9.039548022598871e-06, 'epoch': 2.97}\n{'loss': 0.4507, 'grad_norm': 500067.09375, 'learning_rate': 9.001883239171375e-06, 'epoch': 2.98}\n{'loss': 0.4961, 'grad_norm': 491952.84375, 'learning_rate': 8.96421845574388e-06, 'epoch': 2.99}\n{'loss': 2.254, 'grad_norm': 2765467.25, 'learning_rate': 8.926553672316384e-06, 'epoch': 3.0}\n{'loss': 0.2562, 'grad_norm': 267336.90625, 'learning_rate': 8.888888888888888e-06, 'epoch': 3.01}\n{'loss': 0.2144, 'grad_norm': 235752.109375, 'learning_rate': 8.851224105461394e-06, 'epoch': 3.02}\n{'loss': 0.244, 'grad_norm': 265847.125, 'learning_rate': 8.8135593220339e-06, 'epoch': 3.03}\n{'loss': 0.2602, 'grad_norm': 284178.34375, 'learning_rate': 8.775894538606405e-06, 'epoch': 3.03}\n{'loss': 0.1162, 'grad_norm': 200211.640625, 'learning_rate': 8.738229755178909e-06, 'epoch': 3.04}\n{'loss': 0.2408, 'grad_norm': 321372.8125, 'learning_rate': 8.700564971751413e-06, 'epoch': 3.05}\n{'loss': 0.508, 'grad_norm': 401623.46875, 'learning_rate': 8.662900188323918e-06, 'epoch': 3.06}\n{'loss': 0.08, 'grad_norm': 127480.46875, 'learning_rate': 8.625235404896422e-06, 'epoch': 3.07}\n{'loss': 0.265, 'grad_norm': 267904.0, 'learning_rate': 8.587570621468927e-06, 'epoch': 3.08}\n{'loss': 0.3779, 'grad_norm': 349375.03125, 'learning_rate': 8.549905838041433e-06, 'epoch': 3.08}\n{'loss': 0.1772, 'grad_norm': 217822.1875, 'learning_rate': 8.512241054613937e-06, 'epoch': 3.09}\n{'loss': 0.2925, 'grad_norm': 339152.0625, 'learning_rate': 8.47457627118644e-06, 'epoch': 3.1}\n{'loss': 0.2741, 'grad_norm': 352513.8125, 'learning_rate': 8.436911487758946e-06, 'epoch': 3.11}\n{'loss': 0.1871, 'grad_norm': 494497.3125, 'learning_rate': 8.39924670433145e-06, 'epoch': 3.12}\n{'loss': 0.4801, 'grad_norm': 490119.28125, 'learning_rate': 8.361581920903955e-06, 'epoch': 3.13}\n{'loss': 0.513, 'grad_norm': 772099.125, 'learning_rate': 8.323917137476461e-06, 'epoch': 3.14}\n{'loss': 0.1226, 'grad_norm': 340364.09375, 'learning_rate': 8.286252354048965e-06, 'epoch': 3.14}\n{'loss': 0.2887, 'grad_norm': 408170.46875, 'learning_rate': 8.248587570621469e-06, 'epoch': 3.15}\n{'loss': 0.1518, 'grad_norm': 169987.140625, 'learning_rate': 8.210922787193974e-06, 'epoch': 3.16}\n{'loss': 0.2537, 'grad_norm': 533696.1875, 'learning_rate': 8.17325800376648e-06, 'epoch': 3.17}\n{'loss': 0.3482, 'grad_norm': 323129.3125, 'learning_rate': 8.135593220338983e-06, 'epoch': 3.18}\n{'loss': 0.4261, 'grad_norm': 248443.1875, 'learning_rate': 8.097928436911489e-06, 'epoch': 3.19}\n{'loss': 0.194, 'grad_norm': 256763.640625, 'learning_rate': 8.060263653483993e-06, 'epoch': 3.19}\n{'loss': 0.2116, 'grad_norm': 293379.15625, 'learning_rate': 8.022598870056498e-06, 'epoch': 3.2}\n{'loss': 0.2234, 'grad_norm': 280193.25, 'learning_rate': 7.984934086629002e-06, 'epoch': 3.21}\n{'loss': 0.3381, 'grad_norm': 954140.375, 'learning_rate': 7.947269303201508e-06, 'epoch': 3.22}\n{'loss': 0.5811, 'grad_norm': 1200270.5, 'learning_rate': 7.909604519774012e-06, 'epoch': 3.23}\n{'loss': 0.3282, 'grad_norm': 195585.71875, 'learning_rate': 7.871939736346517e-06, 'epoch': 3.24}\n{'loss': 0.3306, 'grad_norm': 393192.1875, 'learning_rate': 7.834274952919021e-06, 'epoch': 3.25}\n{'loss': 0.2569, 'grad_norm': 346855.6875, 'learning_rate': 7.796610169491526e-06, 'epoch': 3.25}\n{'loss': 0.6018, 'grad_norm': 640878.9375, 'learning_rate': 7.75894538606403e-06, 'epoch': 3.26}\n{'loss': 0.3075, 'grad_norm': 618410.0625, 'learning_rate': 7.721280602636536e-06, 'epoch': 3.27}\n{'loss': 0.1464, 'grad_norm': 471427.625, 'learning_rate': 7.68361581920904e-06, 'epoch': 3.28}\n{'loss': 0.4454, 'grad_norm': 302228.1875, 'learning_rate': 7.645951035781545e-06, 'epoch': 3.29}\n{'loss': 0.2976, 'grad_norm': 282949.5, 'learning_rate': 7.608286252354049e-06, 'epoch': 3.3}\n{'loss': 0.2227, 'grad_norm': 556637.375, 'learning_rate': 7.5706214689265545e-06, 'epoch': 3.31}\n{'loss': 0.3523, 'grad_norm': 377792.6875, 'learning_rate': 7.532956685499059e-06, 'epoch': 3.31}\n{'loss': 0.6354, 'grad_norm': 508517.4375, 'learning_rate': 7.495291902071564e-06, 'epoch': 3.32}\n{'loss': 0.3202, 'grad_norm': 269605.625, 'learning_rate': 7.4576271186440685e-06, 'epoch': 3.33}\n{'loss': 0.6005, 'grad_norm': 540242.5625, 'learning_rate': 7.419962335216573e-06, 'epoch': 3.34}\n{'loss': 0.5199, 'grad_norm': 231978.75, 'learning_rate': 7.382297551789079e-06, 'epoch': 3.35}\n{'loss': 0.2009, 'grad_norm': 279997.28125, 'learning_rate': 7.3446327683615825e-06, 'epoch': 3.36}\n{'loss': 0.5717, 'grad_norm': 416296.0625, 'learning_rate': 7.306967984934087e-06, 'epoch': 3.36}\n{'loss': 0.1009, 'grad_norm': 217197.296875, 'learning_rate': 7.269303201506593e-06, 'epoch': 3.37}\n{'loss': 0.097, 'grad_norm': 149860.1875, 'learning_rate': 7.2316384180790965e-06, 'epoch': 3.38}\n{'loss': 0.2592, 'grad_norm': 461039.46875, 'learning_rate': 7.193973634651601e-06, 'epoch': 3.39}\n{'loss': 0.122, 'grad_norm': 277266.5625, 'learning_rate': 7.156308851224107e-06, 'epoch': 3.4}\n{'loss': 0.0822, 'grad_norm': 248610.234375, 'learning_rate': 7.1186440677966106e-06, 'epoch': 3.41}\n{'loss': 0.0455, 'grad_norm': 80100.453125, 'learning_rate': 7.080979284369115e-06, 'epoch': 3.42}\n{'loss': 0.3536, 'grad_norm': 326465.0625, 'learning_rate': 7.043314500941621e-06, 'epoch': 3.42}\n{'loss': 0.4878, 'grad_norm': 806630.0, 'learning_rate': 7.0056497175141246e-06, 'epoch': 3.43}\n{'loss': 0.4361, 'grad_norm': 559563.625, 'learning_rate': 6.96798493408663e-06, 'epoch': 3.44}\n{'loss': 0.1743, 'grad_norm': 581765.375, 'learning_rate': 6.930320150659135e-06, 'epoch': 3.45}\n{'loss': 0.3442, 'grad_norm': 410638.75, 'learning_rate': 6.892655367231639e-06, 'epoch': 3.46}\n{'loss': 0.2512, 'grad_norm': 491170.78125, 'learning_rate': 6.854990583804144e-06, 'epoch': 3.47}\n{'loss': 0.2999, 'grad_norm': 814827.4375, 'learning_rate': 6.817325800376649e-06, 'epoch': 3.47}\n{'loss': 0.4875, 'grad_norm': 333674.03125, 'learning_rate': 6.779661016949153e-06, 'epoch': 3.48}\n{'loss': 0.1821, 'grad_norm': 450593.125, 'learning_rate': 6.741996233521658e-06, 'epoch': 3.49}\n{'loss': 0.3175, 'grad_norm': 307127.96875, 'learning_rate': 6.704331450094163e-06, 'epoch': 3.5}\n{'loss': 0.3407, 'grad_norm': 279086.1875, 'learning_rate': 6.666666666666667e-06, 'epoch': 3.51}\n{'loss': 0.3648, 'grad_norm': 301566.96875, 'learning_rate': 6.629001883239172e-06, 'epoch': 3.52}\n{'loss': 0.2459, 'grad_norm': 636561.5, 'learning_rate': 6.591337099811677e-06, 'epoch': 3.53}\n{'loss': 0.4384, 'grad_norm': 432280.28125, 'learning_rate': 6.553672316384181e-06, 'epoch': 3.53}\n{'loss': 0.2608, 'grad_norm': 353289.8125, 'learning_rate': 6.516007532956686e-06, 'epoch': 3.54}\n{'loss': 0.1381, 'grad_norm': 357559.1875, 'learning_rate': 6.478342749529191e-06, 'epoch': 3.55}\n{'loss': 0.0888, 'grad_norm': 218635.421875, 'learning_rate': 6.440677966101695e-06, 'epoch': 3.56}\n{'loss': 0.1119, 'grad_norm': 476658.5625, 'learning_rate': 6.4030131826742e-06, 'epoch': 3.57}\n{'loss': 0.2009, 'grad_norm': 435775.53125, 'learning_rate': 6.365348399246705e-06, 'epoch': 3.58}\n{'loss': 0.2835, 'grad_norm': 366206.0625, 'learning_rate': 6.32768361581921e-06, 'epoch': 3.58}\n{'loss': 0.4571, 'grad_norm': 706134.5625, 'learning_rate': 6.290018832391714e-06, 'epoch': 3.59}\n{'loss': 0.6163, 'grad_norm': 574645.9375, 'learning_rate': 6.252354048964219e-06, 'epoch': 3.6}\n{'loss': 0.6073, 'grad_norm': 590956.3125, 'learning_rate': 6.2146892655367244e-06, 'epoch': 3.61}\n{'loss': 0.5946, 'grad_norm': 651336.875, 'learning_rate': 6.177024482109228e-06, 'epoch': 3.62}\n{'loss': 0.2684, 'grad_norm': 558219.4375, 'learning_rate': 6.139359698681733e-06, 'epoch': 3.63}\n{'loss': 0.195, 'grad_norm': 404546.09375, 'learning_rate': 6.1016949152542385e-06, 'epoch': 3.64}\n{'loss': 0.376, 'grad_norm': 565215.9375, 'learning_rate': 6.064030131826742e-06, 'epoch': 3.64}\n{'loss': 0.2489, 'grad_norm': 281315.28125, 'learning_rate': 6.026365348399247e-06, 'epoch': 3.65}\n{'loss': 0.0797, 'grad_norm': 131050.0859375, 'learning_rate': 5.9887005649717525e-06, 'epoch': 3.66}\n{'loss': 0.1579, 'grad_norm': 258731.984375, 'learning_rate': 5.951035781544256e-06, 'epoch': 3.67}\n{'loss': 0.1643, 'grad_norm': 205267.40625, 'learning_rate': 5.913370998116761e-06, 'epoch': 3.68}\n{'loss': 0.3406, 'grad_norm': 323730.34375, 'learning_rate': 5.8757062146892665e-06, 'epoch': 3.69}\n{'loss': 0.3202, 'grad_norm': 394852.75, 'learning_rate': 5.83804143126177e-06, 'epoch': 3.69}\n{'loss': 0.2934, 'grad_norm': 614260.8125, 'learning_rate': 5.800376647834275e-06, 'epoch': 3.7}\n{'loss': 0.3311, 'grad_norm': 439403.1875, 'learning_rate': 5.7627118644067805e-06, 'epoch': 3.71}\n{'loss': 0.2423, 'grad_norm': 407027.375, 'learning_rate': 5.725047080979284e-06, 'epoch': 3.72}\n{'loss': 0.2678, 'grad_norm': 731345.875, 'learning_rate': 5.68738229755179e-06, 'epoch': 3.73}\n{'loss': 0.3101, 'grad_norm': 416932.125, 'learning_rate': 5.6497175141242946e-06, 'epoch': 3.74}\n{'loss': 0.2214, 'grad_norm': 316794.375, 'learning_rate': 5.612052730696798e-06, 'epoch': 3.75}\n{'loss': 0.375, 'grad_norm': 735853.4375, 'learning_rate': 5.574387947269304e-06, 'epoch': 3.75}\n{'loss': 0.0908, 'grad_norm': 175497.6875, 'learning_rate': 5.536723163841809e-06, 'epoch': 3.76}\n{'loss': 0.2549, 'grad_norm': 277991.78125, 'learning_rate': 5.499058380414312e-06, 'epoch': 3.77}\n{'loss': 0.0867, 'grad_norm': 221679.875, 'learning_rate': 5.461393596986818e-06, 'epoch': 3.78}\n{'loss': 0.3809, 'grad_norm': 453466.21875, 'learning_rate': 5.423728813559323e-06, 'epoch': 3.79}\n{'loss': 0.1341, 'grad_norm': 346069.5, 'learning_rate': 5.3860640301318264e-06, 'epoch': 3.8}\n{'loss': 0.2015, 'grad_norm': 403364.84375, 'learning_rate': 5.348399246704332e-06, 'epoch': 3.81}\n{'loss': 0.1844, 'grad_norm': 323454.09375, 'learning_rate': 5.310734463276837e-06, 'epoch': 3.81}\n{'loss': 0.7514, 'grad_norm': 825978.5, 'learning_rate': 5.2730696798493405e-06, 'epoch': 3.82}\n{'loss': 0.4662, 'grad_norm': 910133.625, 'learning_rate': 5.235404896421846e-06, 'epoch': 3.83}\n{'loss': 0.5082, 'grad_norm': 586580.3125, 'learning_rate': 5.197740112994351e-06, 'epoch': 3.84}\n{'loss': 0.1271, 'grad_norm': 380614.78125, 'learning_rate': 5.1600753295668545e-06, 'epoch': 3.85}\n{'loss': 0.4024, 'grad_norm': 503852.21875, 'learning_rate': 5.12241054613936e-06, 'epoch': 3.86}\n{'loss': 0.1364, 'grad_norm': 296544.25, 'learning_rate': 5.084745762711865e-06, 'epoch': 3.86}\n{'loss': 0.4524, 'grad_norm': 524455.875, 'learning_rate': 5.04708097928437e-06, 'epoch': 3.87}\n{'loss': 0.1393, 'grad_norm': 229890.703125, 'learning_rate': 5.009416195856874e-06, 'epoch': 3.88}\n{'loss': 0.1651, 'grad_norm': 370388.375, 'learning_rate': 4.9717514124293796e-06, 'epoch': 3.89}\n{'loss': 0.1066, 'grad_norm': 320517.59375, 'learning_rate': 4.934086629001883e-06, 'epoch': 3.9}\n{'loss': 0.2445, 'grad_norm': 698706.1875, 'learning_rate': 4.896421845574388e-06, 'epoch': 3.91}\n{'loss': 0.435, 'grad_norm': 482591.96875, 'learning_rate': 4.8587570621468936e-06, 'epoch': 3.92}\n{'loss': 0.1182, 'grad_norm': 399631.625, 'learning_rate': 4.821092278719397e-06, 'epoch': 3.92}\n{'loss': 0.1245, 'grad_norm': 313748.125, 'learning_rate': 4.783427495291902e-06, 'epoch': 3.93}\n{'loss': 0.8337, 'grad_norm': 660578.75, 'learning_rate': 4.745762711864408e-06, 'epoch': 3.94}\n{'loss': 0.2628, 'grad_norm': 647010.75, 'learning_rate': 4.7080979284369114e-06, 'epoch': 3.95}\n{'loss': 0.281, 'grad_norm': 1093746.25, 'learning_rate': 4.670433145009416e-06, 'epoch': 3.96}\n{'loss': 0.2275, 'grad_norm': 521939.46875, 'learning_rate': 4.632768361581922e-06, 'epoch': 3.97}\n{'loss': 0.1229, 'grad_norm': 466757.90625, 'learning_rate': 4.595103578154426e-06, 'epoch': 3.97}\n{'loss': 0.3643, 'grad_norm': 392490.28125, 'learning_rate': 4.55743879472693e-06, 'epoch': 3.98}\n{'loss': 0.3175, 'grad_norm': 418442.5625, 'learning_rate': 4.519774011299436e-06, 'epoch': 3.99}\n{'loss': 0.0937, 'grad_norm': 469253.1875, 'learning_rate': 4.48210922787194e-06, 'epoch': 4.0}\n{'loss': 0.1147, 'grad_norm': 245329.078125, 'learning_rate': 4.444444444444444e-06, 'epoch': 4.01}\n{'loss': 0.0646, 'grad_norm': 247930.703125, 'learning_rate': 4.40677966101695e-06, 'epoch': 4.02}\n{'loss': 0.1462, 'grad_norm': 431277.71875, 'learning_rate': 4.369114877589454e-06, 'epoch': 4.03}\n{'loss': 0.1943, 'grad_norm': 265253.90625, 'learning_rate': 4.331450094161959e-06, 'epoch': 4.03}\n{'loss': 0.2287, 'grad_norm': 514576.5625, 'learning_rate': 4.293785310734464e-06, 'epoch': 4.04}\n{'loss': 0.4023, 'grad_norm': 245163.921875, 'learning_rate': 4.256120527306968e-06, 'epoch': 4.05}\n{'loss': 0.0591, 'grad_norm': 140077.203125, 'learning_rate': 4.218455743879473e-06, 'epoch': 4.06}\n{'loss': 0.1229, 'grad_norm': 588335.25, 'learning_rate': 4.180790960451978e-06, 'epoch': 4.07}\n{'loss': 0.2093, 'grad_norm': 423755.9375, 'learning_rate': 4.143126177024482e-06, 'epoch': 4.08}\n{'loss': 0.1042, 'grad_norm': 256704.59375, 'learning_rate': 4.105461393596987e-06, 'epoch': 4.08}\n{'loss': 0.0669, 'grad_norm': 155883.203125, 'learning_rate': 4.067796610169492e-06, 'epoch': 4.09}\n{'loss': 0.1859, 'grad_norm': 215886.984375, 'learning_rate': 4.030131826741996e-06, 'epoch': 4.1}\n{'loss': 0.0717, 'grad_norm': 225788.4375, 'learning_rate': 3.992467043314501e-06, 'epoch': 4.11}\n{'loss': 0.4115, 'grad_norm': 422129.5625, 'learning_rate': 3.954802259887006e-06, 'epoch': 4.12}\n{'loss': 0.3616, 'grad_norm': 501630.625, 'learning_rate': 3.9171374764595104e-06, 'epoch': 4.13}\n{'loss': 0.0773, 'grad_norm': 193701.375, 'learning_rate': 3.879472693032015e-06, 'epoch': 4.14}\n{'loss': 0.5299, 'grad_norm': 828744.375, 'learning_rate': 3.84180790960452e-06, 'epoch': 4.14}\n{'loss': 0.0968, 'grad_norm': 392171.40625, 'learning_rate': 3.8041431261770245e-06, 'epoch': 4.15}\n{'loss': 0.2605, 'grad_norm': 370723.1875, 'learning_rate': 3.7664783427495296e-06, 'epoch': 4.16}\n{'loss': 0.2238, 'grad_norm': 380735.3125, 'learning_rate': 3.7288135593220342e-06, 'epoch': 4.17}\n{'loss': 0.1051, 'grad_norm': 595692.75, 'learning_rate': 3.6911487758945393e-06, 'epoch': 4.18}\n{'loss': 0.2293, 'grad_norm': 618384.875, 'learning_rate': 3.6534839924670436e-06, 'epoch': 4.19}\n{'loss': 0.3779, 'grad_norm': 569102.4375, 'learning_rate': 3.6158192090395483e-06, 'epoch': 4.19}\n{'loss': 0.0957, 'grad_norm': 332945.90625, 'learning_rate': 3.5781544256120534e-06, 'epoch': 4.2}\n{'loss': 0.0874, 'grad_norm': 298081.625, 'learning_rate': 3.5404896421845576e-06, 'epoch': 4.21}\n{'loss': 0.2972, 'grad_norm': 216388.546875, 'learning_rate': 3.5028248587570623e-06, 'epoch': 4.22}\n{'loss': 0.3538, 'grad_norm': 485887.5, 'learning_rate': 3.4651600753295674e-06, 'epoch': 4.23}\n{'loss': 0.8579, 'grad_norm': 581763.75, 'learning_rate': 3.427495291902072e-06, 'epoch': 4.24}\n{'loss': 0.2006, 'grad_norm': 318918.6875, 'learning_rate': 3.3898305084745763e-06, 'epoch': 4.25}\n{'loss': 0.0461, 'grad_norm': 190303.5625, 'learning_rate': 3.3521657250470814e-06, 'epoch': 4.25}\n{'loss': 0.2614, 'grad_norm': 140053.125, 'learning_rate': 3.314500941619586e-06, 'epoch': 4.26}\n{'loss': 0.3836, 'grad_norm': 438068.8125, 'learning_rate': 3.2768361581920903e-06, 'epoch': 4.27}\n{'loss': 0.1551, 'grad_norm': 479462.34375, 'learning_rate': 3.2391713747645954e-06, 'epoch': 4.28}\n{'loss': 0.1531, 'grad_norm': 495067.84375, 'learning_rate': 3.2015065913371e-06, 'epoch': 4.29}\n{'loss': 0.0988, 'grad_norm': 656472.75, 'learning_rate': 3.163841807909605e-06, 'epoch': 4.3}\n{'loss': 0.0881, 'grad_norm': 242719.921875, 'learning_rate': 3.1261770244821095e-06, 'epoch': 4.31}\n{'loss': 0.3571, 'grad_norm': 307730.875, 'learning_rate': 3.088512241054614e-06, 'epoch': 4.31}\n{'loss': 0.0571, 'grad_norm': 112591.78125, 'learning_rate': 3.0508474576271192e-06, 'epoch': 4.32}\n{'loss': 0.3658, 'grad_norm': 285019.3125, 'learning_rate': 3.0131826741996235e-06, 'epoch': 4.33}\n{'loss': 0.3776, 'grad_norm': 614318.0625, 'learning_rate': 2.975517890772128e-06, 'epoch': 4.34}\n{'loss': 0.2644, 'grad_norm': 316100.96875, 'learning_rate': 2.9378531073446333e-06, 'epoch': 4.35}\n{'loss': 0.0981, 'grad_norm': 388997.84375, 'learning_rate': 2.9001883239171375e-06, 'epoch': 4.36}\n{'loss': 0.0875, 'grad_norm': 247471.28125, 'learning_rate': 2.862523540489642e-06, 'epoch': 4.36}\n{'loss': 0.135, 'grad_norm': 404991.15625, 'learning_rate': 2.8248587570621473e-06, 'epoch': 4.37}\n{'loss': 0.1342, 'grad_norm': 1092870.875, 'learning_rate': 2.787193973634652e-06, 'epoch': 4.38}\n{'loss': 0.2366, 'grad_norm': 362211.0, 'learning_rate': 2.749529190207156e-06, 'epoch': 4.39}\n{'loss': 0.2562, 'grad_norm': 382605.0, 'learning_rate': 2.7118644067796613e-06, 'epoch': 4.4}\n{'loss': 0.2579, 'grad_norm': 363508.40625, 'learning_rate': 2.674199623352166e-06, 'epoch': 4.41}\n{'loss': 0.2232, 'grad_norm': 583918.5625, 'learning_rate': 2.6365348399246702e-06, 'epoch': 4.42}\n{'loss': 0.5313, 'grad_norm': 374015.84375, 'learning_rate': 2.5988700564971753e-06, 'epoch': 4.42}\n{'loss': 0.3367, 'grad_norm': 658520.75, 'learning_rate': 2.56120527306968e-06, 'epoch': 4.43}\n{'loss': 0.1304, 'grad_norm': 425411.6875, 'learning_rate': 2.523540489642185e-06, 'epoch': 4.44}\n{'loss': 0.127, 'grad_norm': 545875.8125, 'learning_rate': 2.4858757062146898e-06, 'epoch': 4.45}\n{'loss': 0.3983, 'grad_norm': 543369.25, 'learning_rate': 2.448210922787194e-06, 'epoch': 4.46}\n{'loss': 0.1426, 'grad_norm': 351624.8125, 'learning_rate': 2.4105461393596987e-06, 'epoch': 4.47}\n{'loss': 0.141, 'grad_norm': 362269.09375, 'learning_rate': 2.372881355932204e-06, 'epoch': 4.47}\n{'loss': 0.2022, 'grad_norm': 420501.25, 'learning_rate': 2.335216572504708e-06, 'epoch': 4.48}\n{'loss': 0.1422, 'grad_norm': 311335.03125, 'learning_rate': 2.297551789077213e-06, 'epoch': 4.49}\n{'loss': 0.1253, 'grad_norm': 464668.25, 'learning_rate': 2.259887005649718e-06, 'epoch': 4.5}\n{'loss': 0.1378, 'grad_norm': 250202.59375, 'learning_rate': 2.222222222222222e-06, 'epoch': 4.51}\n{'loss': 0.0323, 'grad_norm': 56122.0234375, 'learning_rate': 2.184557438794727e-06, 'epoch': 4.52}\n{'loss': 0.2001, 'grad_norm': 611462.75, 'learning_rate': 2.146892655367232e-06, 'epoch': 4.53}\n{'loss': 0.1427, 'grad_norm': 997046.625, 'learning_rate': 2.1092278719397365e-06, 'epoch': 4.53}\n{'loss': 0.2536, 'grad_norm': 417743.875, 'learning_rate': 2.071563088512241e-06, 'epoch': 4.54}\n{'loss': 0.0598, 'grad_norm': 138728.203125, 'learning_rate': 2.033898305084746e-06, 'epoch': 4.55}\n{'loss': 0.4584, 'grad_norm': 318903.8125, 'learning_rate': 1.9962335216572505e-06, 'epoch': 4.56}\n{'loss': 0.2324, 'grad_norm': 511796.53125, 'learning_rate': 1.9585687382297552e-06, 'epoch': 4.57}\n{'loss': 0.1902, 'grad_norm': 654198.9375, 'learning_rate': 1.92090395480226e-06, 'epoch': 4.58}\n{'loss': 0.0631, 'grad_norm': 222404.046875, 'learning_rate': 1.8832391713747648e-06, 'epoch': 4.58}\n{'loss': 0.1355, 'grad_norm': 406661.0625, 'learning_rate': 1.8455743879472697e-06, 'epoch': 4.59}\n{'loss': 0.3976, 'grad_norm': 368256.5, 'learning_rate': 1.8079096045197741e-06, 'epoch': 4.6}\n{'loss': 0.0742, 'grad_norm': 272541.75, 'learning_rate': 1.7702448210922788e-06, 'epoch': 4.61}\n{'loss': 0.7712, 'grad_norm': 598600.1875, 'learning_rate': 1.7325800376647837e-06, 'epoch': 4.62}\n{'loss': 0.1575, 'grad_norm': 217081.171875, 'learning_rate': 1.6949152542372882e-06, 'epoch': 4.63}\n{'loss': 0.1641, 'grad_norm': 453161.78125, 'learning_rate': 1.657250470809793e-06, 'epoch': 4.64}\n{'loss': 0.0313, 'grad_norm': 83450.6171875, 'learning_rate': 1.6195856873822977e-06, 'epoch': 4.64}\n{'loss': 0.1394, 'grad_norm': 744797.6875, 'learning_rate': 1.5819209039548026e-06, 'epoch': 4.65}\n{'loss': 0.3114, 'grad_norm': 595380.875, 'learning_rate': 1.544256120527307e-06, 'epoch': 4.66}\n{'loss': 0.3693, 'grad_norm': 657101.125, 'learning_rate': 1.5065913370998117e-06, 'epoch': 4.67}\n{'loss': 0.0494, 'grad_norm': 75547.9609375, 'learning_rate': 1.4689265536723166e-06, 'epoch': 4.68}\n{'loss': 0.1118, 'grad_norm': 445783.96875, 'learning_rate': 1.431261770244821e-06, 'epoch': 4.69}\n{'loss': 0.2744, 'grad_norm': 521889.53125, 'learning_rate': 1.393596986817326e-06, 'epoch': 4.69}\n{'loss': 0.0749, 'grad_norm': 260381.609375, 'learning_rate': 1.3559322033898307e-06, 'epoch': 4.7}\n{'loss': 0.3172, 'grad_norm': 389707.3125, 'learning_rate': 1.3182674199623351e-06, 'epoch': 4.71}\n{'loss': 0.7109, 'grad_norm': 331524.6875, 'learning_rate': 1.28060263653484e-06, 'epoch': 4.72}\n{'loss': 0.2204, 'grad_norm': 593114.5625, 'learning_rate': 1.2429378531073449e-06, 'epoch': 4.73}\n{'loss': 0.0733, 'grad_norm': 184223.9375, 'learning_rate': 1.2052730696798494e-06, 'epoch': 4.74}\n{'loss': 0.0628, 'grad_norm': 154737.9375, 'learning_rate': 1.167608286252354e-06, 'epoch': 4.75}\n{'loss': 0.1098, 'grad_norm': 362466.78125, 'learning_rate': 1.129943502824859e-06, 'epoch': 4.75}\n{'loss': 0.2377, 'grad_norm': 487152.28125, 'learning_rate': 1.0922787193973636e-06, 'epoch': 4.76}\n{'loss': 0.3027, 'grad_norm': 1125289.0, 'learning_rate': 1.0546139359698683e-06, 'epoch': 4.77}\n{'loss': 0.0387, 'grad_norm': 62941.27734375, 'learning_rate': 1.016949152542373e-06, 'epoch': 4.78}\n{'loss': 0.045, 'grad_norm': 90899.78125, 'learning_rate': 9.792843691148776e-07, 'epoch': 4.79}\n{'loss': 0.3766, 'grad_norm': 686501.8125, 'learning_rate': 9.416195856873824e-07, 'epoch': 4.8}\n{'loss': 0.1989, 'grad_norm': 279678.0, 'learning_rate': 9.039548022598871e-07, 'epoch': 4.81}\n{'loss': 0.2092, 'grad_norm': 794511.9375, 'learning_rate': 8.662900188323918e-07, 'epoch': 4.81}\n{'loss': 0.3482, 'grad_norm': 800713.25, 'learning_rate': 8.286252354048965e-07, 'epoch': 4.82}\n{'loss': 0.2887, 'grad_norm': 187235.359375, 'learning_rate': 7.909604519774013e-07, 'epoch': 4.83}\n{'loss': 0.1868, 'grad_norm': 632716.1875, 'learning_rate': 7.532956685499059e-07, 'epoch': 4.84}\n{'loss': 0.5051, 'grad_norm': 466220.375, 'learning_rate': 7.156308851224105e-07, 'epoch': 4.85}\n{'loss': 0.1091, 'grad_norm': 473527.8125, 'learning_rate': 6.779661016949153e-07, 'epoch': 4.86}\n{'loss': 0.3608, 'grad_norm': 628528.8125, 'learning_rate': 6.4030131826742e-07, 'epoch': 4.86}\n{'loss': 0.1261, 'grad_norm': 319362.09375, 'learning_rate': 6.026365348399247e-07, 'epoch': 4.87}\n{'loss': 0.0487, 'grad_norm': 135829.25, 'learning_rate': 5.649717514124295e-07, 'epoch': 4.88}\n{'loss': 0.0522, 'grad_norm': 153162.96875, 'learning_rate': 5.273069679849341e-07, 'epoch': 4.89}\n{'loss': 0.1928, 'grad_norm': 657341.5625, 'learning_rate': 4.896421845574388e-07, 'epoch': 4.9}\n{'loss': 0.3111, 'grad_norm': 724454.75, 'learning_rate': 4.5197740112994353e-07, 'epoch': 4.91}\n{'loss': 0.3428, 'grad_norm': 730941.125, 'learning_rate': 4.1431261770244826e-07, 'epoch': 4.92}\n{'loss': 0.0374, 'grad_norm': 131684.859375, 'learning_rate': 3.7664783427495294e-07, 'epoch': 4.92}\n{'loss': 0.1892, 'grad_norm': 453167.4375, 'learning_rate': 3.3898305084745766e-07, 'epoch': 4.93}\n{'loss': 0.0738, 'grad_norm': 356476.3125, 'learning_rate': 3.0131826741996234e-07, 'epoch': 4.94}\n{'loss': 0.0626, 'grad_norm': 357781.25, 'learning_rate': 2.6365348399246707e-07, 'epoch': 4.95}\n{'loss': 0.0535, 'grad_norm': 128300.515625, 'learning_rate': 2.2598870056497177e-07, 'epoch': 4.96}\n{'loss': 0.1558, 'grad_norm': 662760.5625, 'learning_rate': 1.8832391713747647e-07, 'epoch': 4.97}\n{'loss': 0.1204, 'grad_norm': 297964.1875, 'learning_rate': 1.5065913370998117e-07, 'epoch': 4.97}\n{'loss': 0.2178, 'grad_norm': 765789.875, 'learning_rate': 1.1299435028248588e-07, 'epoch': 4.98}\n{'loss': 0.1018, 'grad_norm': 451905.40625, 'learning_rate': 7.532956685499058e-08, 'epoch': 4.99}\n{'loss': 0.0242, 'grad_norm': 47314.859375, 'learning_rate': 3.766478342749529e-08, 'epoch': 5.0}\n{'train_runtime': 394.7359, 'train_samples_per_second': 23.75, 'train_steps_per_second': 1.495, 'train_loss': 0.4301107012335274, 'epoch': 5.0}\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 590/590 [06:34<00:00,  1.49it/s]\nTraining complete.\n\nLoading test data for seed=42...\nRunning prediction...\n100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 2786.91it/s]\nðŸ“¤ Saved submission to: submission_deberta_42.csv\n","output_type":"stream"}],"execution_count":34},{"cell_type":"markdown","source":"# Inference","metadata":{}},{"cell_type":"markdown","source":"## (1) RoBERTa","metadata":{}},{"cell_type":"code","source":"%%writefile inference_roberta.py\n\nimport argparse, pandas as pd\nfrom constants import BATCH_SIZE, MAX_TOKEN, ROBERTA_MODEL_DIR, OUT_DIR, SEED\nfrom utils_bert import data_processing\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import (\n    AutoTokenizer, AutoConfig, AutoModelForSequenceClassification,\n    DataCollatorWithPadding\n)\n\n\ndevice = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n\ndef tokenize_test(tokenizer, comments, max_length):\n    input_ids, attention_masks = [], []\n    for comment in comments:\n        enc = tokenizer.encode_plus(\n            comment,\n            add_special_tokens=True,\n            truncation=True,\n            max_length=max_length,\n            padding=\"max_length\",\n            return_attention_mask=True,\n            return_tensors=\"pt\",\n        )\n        input_ids.append(enc[\"input_ids\"])\n        attention_masks.append(enc[\"attention_mask\"])\n    return torch.cat(input_ids, dim=0), torch.cat(attention_masks, dim=0)\n\n\ndef predict(model, loader, device):\n    model.eval()\n    all_preds = []\n    all_probs = []\n    \n    with torch.no_grad():\n        for batch in loader:\n            input_ids, attention_mask = [t.to(device) for t in batch]\n            \n            output = model(input_ids=input_ids, attention_mask=attention_mask)\n\n            logits = output.logits.squeeze(-1)\n            probs = torch.sigmoid(logits)\n            all_probs.extend(probs.cpu().tolist())\n            \n    return all_probs\n\n\ndef main():\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--model\", type=str, required=True)\n    parser.add_argument(\"--save\", type=str, required=True)\n    args = parser.parse_args()\n    \n    df_test = pd.read_csv(\"/kaggle/input/jigsaw-agile-community-rules/test.csv\")\n    submission = pd.DataFrame(df_test[\"row_id\"])\n    df_test = data_processing(df_test, is_train=False)\n\n    # Tokenizer\n    tokenizer = AutoTokenizer.from_pretrained(\n        ROBERTA_MODEL_DIR,\n        do_lower_case=True, \n        local_files_only=True\n    )  \n    \n    # Config\n    config = AutoConfig.from_pretrained(\n        ROBERTA_MODEL_DIR,\n        num_labels=1,  # 1 --> regression (MSE Loss), 2--> binary classification (Cross-entropy)\n        problem_type=\"single_label_classification\",\n        local_files_only=True,\n        hidden_dropout_prob=0.2,\n        attention_probs_dropout_prob=0.2,\n    )\n    \n    # Model\n    model = AutoModelForSequenceClassification.from_pretrained(\n        ROBERTA_MODEL_DIR,\n        config=config,\n        local_files_only=True\n    )\n\n    # loading weight from the trained model\n    model.load_state_dict(torch.load(args.model, map_location=device))\n    model.to(device)\n\n    test_input_ids, test_attention_masks = tokenize_test(tokenizer, df_test[\"data\"], max_length=MAX_TOKEN)\n    test_dataset = TensorDataset(test_input_ids, test_attention_masks)\n    test_loader  = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n\n    res = predict(model, test_loader, device)\n    submission[\"rule_violation\"] = res\n    submission.to_csv(args.save, index=False)\n    print(submission)\n    \nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:32:07.484716Z","iopub.execute_input":"2025-10-25T14:32:07.485234Z","iopub.status.idle":"2025-10-25T14:32:07.491534Z","shell.execute_reply.started":"2025-10-25T14:32:07.485208Z","shell.execute_reply":"2025-10-25T14:32:07.490720Z"}},"outputs":[{"name":"stdout","text":"Overwriting inference_roberta.py\n","output_type":"stream"}],"execution_count":28},{"cell_type":"code","source":"!python inference_roberta.py --model /kaggle/working/roberta_1 --save /kaggle/working/submission_roberta_1.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:32:11.555367Z","iopub.execute_input":"2025-10-25T14:32:11.555703Z","iopub.status.idle":"2025-10-25T14:32:25.586860Z","shell.execute_reply.started":"2025-10-25T14:32:11.555679Z","shell.execute_reply":"2025-10-25T14:32:25.585999Z"}},"outputs":[{"name":"stdout","text":"2025-10-25 14:32:17.159337: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761402737.182583     152 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761402737.189923     152 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n   row_id  rule_violation\n0    2029        0.009641\n1    2030        0.918124\n2    2031        0.988647\n3    2032        0.979562\n4    2033        0.989395\n5    2034        0.046523\n6    2035        0.927087\n7    2036        0.014510\n8    2037        0.011171\n9    2038        0.982906\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"!python inference_roberta.py --model /kaggle/working/roberta_2 --save /kaggle/working/submission_roberta_2.csv","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:32:32.716621Z","iopub.execute_input":"2025-10-25T14:32:32.717256Z","iopub.status.idle":"2025-10-25T14:32:46.982000Z","shell.execute_reply.started":"2025-10-25T14:32:32.717231Z","shell.execute_reply":"2025-10-25T14:32:46.981218Z"}},"outputs":[{"name":"stdout","text":"2025-10-25 14:32:38.451358: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1761402758.475823     170 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1761402758.484059     170 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nSome weights of RobertaForSequenceClassification were not initialized from the model checkpoint at /kaggle/input/roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n   row_id  rule_violation\n0    2029        0.005480\n1    2030        0.129433\n2    2031        0.989366\n3    2032        0.996093\n4    2033        0.996885\n5    2034        0.005832\n6    2035        0.985622\n7    2036        0.004635\n8    2037        0.008519\n9    2038        0.995712\n","output_type":"stream"}],"execution_count":30},{"cell_type":"markdown","source":"## (3) deberta-v3-base","metadata":{}},{"cell_type":"code","source":"res_deberta = pd.read_csv(\"/kaggle/working/submission_deberta_42.csv\")\nres_deberta","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:21:20.839163Z","iopub.execute_input":"2025-10-25T14:21:20.839426Z","iopub.status.idle":"2025-10-25T14:21:20.843164Z","shell.execute_reply.started":"2025-10-25T14:21:20.839403Z","shell.execute_reply":"2025-10-25T14:21:20.842611Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# Ensemble","metadata":{}},{"cell_type":"code","source":"df1 = pd.read_csv(\"/kaggle/working/submission_roberta_1.csv\")\ndf2 = pd.read_csv(\"/kaggle/working/submission_roberta_2.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:33:32.416266Z","iopub.execute_input":"2025-10-25T14:33:32.417365Z","iopub.status.idle":"2025-10-25T14:33:32.426572Z","shell.execute_reply.started":"2025-10-25T14:33:32.417332Z","shell.execute_reply":"2025-10-25T14:33:32.425621Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import rankdata\n\ndf1 = pd.read_csv(\"/kaggle/working/submission_roberta_1.csv\")\ndf2 = pd.read_csv(\"/kaggle/working/submission_roberta_2.csv\")\ndf3 = pd.read_csv(\"/kaggle/working/submission_deberta_42.csv\")\n\nr1 = rankdata(df1[\"rule_violation\"])\nr2 = rankdata(df2[\"rule_violation\"])\nr3 = rankdata(df3[\"rule_violation\"])\n\nfinal_rank = (r1 + r2 + r3) / 3\nfinal_pred = final_rank / np.max(final_rank) # normalization\n\ndf1[\"rule_violation\"] = final_pred\ndf1.to_csv(\"submission.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:21:20.987161Z","iopub.status.idle":"2025-10-25T14:21:20.987398Z","shell.execute_reply.started":"2025-10-25T14:21:20.987281Z","shell.execute_reply":"2025-10-25T14:21:20.987291Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-25T14:21:20.988664Z","iopub.status.idle":"2025-10-25T14:21:20.988896Z","shell.execute_reply.started":"2025-10-25T14:21:20.988790Z","shell.execute_reply":"2025-10-25T14:21:20.988800Z"}},"outputs":[],"execution_count":null}]}